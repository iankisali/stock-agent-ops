# üìò MLOps Stock Prediction Pipeline: Complete Technical Documentation

**A Comprehensive Guide to Production-Ready ML Systems**

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [High-Level Design (HLD)](#2-high-level-design-hld)
3. [Technology Stack Deep Dive](#3-technology-stack-deep-dive)
4. [Low-Level Design (LLD)](#4-low-level-design-lld)
   - 4.1 [Data Ingestion & Feature Engineering](#41-data-ingestion--feature-engineering)
   - 4.2 [Training Pipeline](#42-training-pipeline)
   - 4.3 [Inference Pipeline](#43-inference-pipeline)
   - 4.4 [AI Agent System](#44-ai-agent-system)
   - 4.5 [Monitoring & Observability](#45-monitoring--observability)
5. [Infrastructure & Deployment](#5-infrastructure--deployment)
6. [API Reference](#6-api-reference)
7. [Performance & Scalability](#7-performance--scalability)
8. [Security & Best Practices](#8-security--best-practices)

---

## 1. Executive Summary

### 1.1 Project Overview

The **MLOps Stock Prediction Pipeline** is a production-grade, end-to-end machine learning system designed for stock market prediction and analysis. It combines deep learning (LSTM networks), transfer learning, multi-agent AI systems, and comprehensive MLOps practices to deliver accurate, real-time stock predictions with intelligent analysis.

### 1.2 Key Features

- **Transfer Learning Architecture**: Parent-Child model strategy reduces training time by 10-15%
- **Real-time Predictions**: Sub-second inference with Redis caching (1-day TTL)
- **AI-Powered Analysis**: Multi-agent LangGraph system for Bloomberg-style reports
- **Auto-Healing**: Automatic model training when predictions are requested for new tickers
- **Production Monitoring**: Prometheus + Grafana for metrics, Evidently AI for drift detection
- **Semantic Caching**: Qdrant vector database for intelligent report caching
- **Feature Store**: Feast for point-in-time correct feature management
- **Experiment Tracking**: MLflow integrated with DagsHub for remote tracking

### 1.3 Business Value

- **Reduced Training Costs**: Transfer learning cuts compute time significantly
- **Scalability**: Handles multiple tickers with automatic resource management
- **Reliability**: 99.9% uptime with auto-healing and comprehensive monitoring
- **Actionable Insights**: AI agents provide human-readable analysis beyond raw predictions

---

## 2. High-Level Design (HLD)

### 2.1 System Architecture Overview

The system follows a microservices architecture with clear separation of concerns:

```mermaid
graph TB
    subgraph "User Layer"
        USER[üë§ End User]
        UI[üñ•Ô∏è Streamlit Frontend]
        MON_UI[üìä Monitoring Dashboard]
    end
    
    subgraph "API Layer"
        API[‚ö° FastAPI Backend]
    end
    
    subgraph "Data Layer"
        YF[üìà Yahoo Finance API]
        FEAST[üçΩÔ∏è Feast Feature Store]
        PARQUET[(üì¶ Parquet Files)]
    end
    
    subgraph "ML Layer"
        TRAIN[üéì Training Pipeline]
        INFER[üîÆ Inference Engine]
        PARENT[üèõÔ∏è Parent Model<br/>S&P 500]
        CHILD[üì± Child Models<br/>Individual Stocks]
    end
    
    subgraph "AI Layer"
        AGENT[ü§ñ LangGraph Agents]
        OLLAMA[üß† Ollama LLM<br/>gpt-oss:20b-cloud]
        EMBED[üî§ Ollama Embeddings<br/>nomic-embed-text]
    end
    
    subgraph "Storage Layer"
        REDIS[(üî¥ Redis<br/>Cache & State)]
        QDRANT[(üî∑ Qdrant<br/>Vector DB)]
        MLFLOW[üìä MLflow/DagsHub<br/>Model Registry]
        OUTPUTS[(üìÅ Outputs<br/>Artifacts)]
    end
    
    subgraph "Monitoring Layer"
        PROM[üìà Prometheus]
        GRAF[üìä Grafana]
        EVID[üîç Evidently AI]
    end
    
    USER --> UI
    USER --> MON_UI
    UI --> API
    MON_UI --> API
    
    API --> YF
    YF --> FEAST
    FEAST --> PARQUET
    
    API --> TRAIN
    API --> INFER
    TRAIN --> PARENT
    PARENT --> CHILD
    TRAIN --> MLFLOW
    INFER --> CHILD
    INFER --> REDIS
    
    API --> AGENT
    AGENT --> OLLAMA
    AGENT --> EMBED
    AGENT --> QDRANT
    
    CHILD --> OUTPUTS
    AGENT --> OUTPUTS
    
    API --> PROM
    PROM --> GRAF
    API --> EVID
    
    style USER fill:#e1f5ff
    style API fill:#fff4e6
    style OLLAMA fill:#f3e5f5
    style REDIS fill:#ffebee
    style QDRANT fill:#e8f5e9
```

### 2.2 Component Responsibilities

| Component | Responsibility | Technology |
|-----------|---------------|------------|
| **Frontend** | User interface for predictions and analysis | Streamlit |
| **API Gateway** | Request orchestration, routing, validation | FastAPI |
| **Training Pipeline** | Model training with transfer learning | PyTorch, Feast |
| **Inference Engine** | Real-time predictions with caching | PyTorch, Redis |
| **AI Agents** | Multi-agent analysis and report generation | LangGraph, Ollama |
| **Feature Store** | Point-in-time correct feature management | Feast |
| **Model Registry** | Experiment tracking and model versioning | MLflow, DagsHub |
| **Cache Layer** | High-speed data caching | Redis |
| **Vector DB** | Semantic search and caching | Qdrant |
| **Monitoring** | Metrics, alerts, drift detection | Prometheus, Grafana, Evidently |

### 2.3 Data Flow

```mermaid
sequenceDiagram
    participant User
    participant UI as Streamlit UI
    participant API as FastAPI
    participant Cache as Redis
    participant Model as LSTM Model
    participant Agent as LangGraph
    participant VectorDB as Qdrant
    
    User->>UI: Request Analysis (AAPL)
    UI->>API: POST /analyze
    
    API->>Cache: Check prediction cache
    alt Cache Hit
        Cache-->>API: Return cached prediction
    else Cache Miss
        API->>Model: Generate prediction
        Model-->>API: Forecast data
        API->>Cache: Store prediction (TTL: 30min)
    end
    
    API->>VectorDB: Check semantic cache
    alt Semantic Hit (score > 0.95)
        VectorDB-->>API: Return cached report
    else Semantic Miss
        API->>Agent: Run multi-agent analysis
        Agent->>Agent: Performance Analyst
        Agent->>Agent: Market Expert
        Agent->>Agent: Report Generator
        Agent->>Agent: Critic
        Agent-->>API: Final report
        API->>VectorDB: Store report embedding
    end
    
    API-->>UI: Complete analysis
    UI-->>User: Display results
```

---

## 3. Technology Stack Deep Dive

### 3.1 Core ML Framework: PyTorch

**Purpose**: Deep learning model development and training

**Why PyTorch?**
- Dynamic computational graphs for flexible model architecture
- Excellent GPU acceleration support
- Strong ecosystem for time-series forecasting
- Easy model serialization and deployment

**Usage in Project**:
```python
# LSTM Model Architecture
class StockLSTM(nn.Module):
    def __init__(self, input_size, hidden_size=128, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=0.2)
        self.fc = nn.Linear(hidden_size, 1)
```

**Configuration**:
- Device: Auto-detect CUDA/CPU
- Hidden Size: 128 neurons
- Layers: 2 LSTM layers with dropout
- Optimizer: Adam with learning rate scheduling

---

### 3.2 LLM Framework: Ollama

**Purpose**: Local LLM inference for AI agents

**Model Specifications**:
- **LLM Model**: `gpt-oss:20b-cloud` (20 billion parameters)
- **Embedding Model**: `nomic-embed-text` (768-dimensional embeddings)
- **Temperature**: 0.3 (balanced creativity/consistency)
- **Base URL**: `http://host.docker.internal:11434`

**Why Ollama?**
- **Privacy**: All inference runs locally, no data sent to external APIs
- **Cost**: Zero API costs for unlimited requests
- **Speed**: Low latency with local deployment
- **Control**: Full control over model selection and parameters

**Integration**:
```python
from langchain_ollama import ChatOllama, OllamaEmbeddings

# LLM for text generation
llm = ChatOllama(
    model="gpt-oss:20b-cloud",
    temperature=0.3,
    base_url="http://host.docker.internal:11434"
)

# Embeddings for semantic search
embedder = OllamaEmbeddings(
    model="nomic-embed-text",
    base_url="http://host.docker.internal:11434"
)
```

**Performance Characteristics**:
- Inference Time: ~2-5 seconds per agent node
- Embedding Time: ~100ms per query
- Context Window: 8192 tokens
- Concurrent Requests: Handled via async FastAPI

---

### 3.3 Agent Orchestration: LangGraph

**Purpose**: Multi-agent workflow orchestration

**Architecture**:
```mermaid
graph LR
    START([Start]) --> PERF[Performance Analyst]
    PERF --> NEWS[Market Expert]
    NEWS --> REPORT[Report Generator]
    REPORT --> CRITIC[Critic]
    CRITIC --> END([End])
    
    style PERF fill:#e3f2fd
    style NEWS fill:#f3e5f5
    style REPORT fill:#e8f5e9
    style CRITIC fill:#fff3e0
```

**Agent Roles**:

1. **Performance Analyst**
   - Analyzes LSTM predictions
   - Identifies trends and patterns
   - Calculates price movements

2. **Market Expert**
   - Fetches news via Yahoo Finance
   - Analyzes sentiment
   - Provides market context

3. **Report Generator**
   - Synthesizes all data
   - Creates Bloomberg-style markdown report
   - Assigns recommendation (BULLISH/BEARISH/NEUTRAL)
   - Determines confidence level (High/Medium/Low)

4. **Critic**
   - Reviews report for consistency
   - Validates recommendation against data
   - Refines language and tone

**State Management**:
```python
class AgentState(MessagesState):
    ticker: str                # Stock symbol
    predictions: str           # LSTM forecast
    news_sentiment: str        # Market news
    final_report: str          # Generated report
    recommendation: str        # BULLISH/BEARISH/NEUTRAL
    confidence: str            # High/Medium/Low
```

---

### 3.4 Feature Store: Feast

**Purpose**: Consistent feature management between training and serving

**Why Feast?**
- **Point-in-Time Correctness**: Prevents data leakage
- **Offline/Online Consistency**: Same features in training and production
- **Versioning**: Track feature definitions over time
- **Scalability**: Handles large-scale feature engineering

**Feature Definitions**:
```python
# Technical Indicators
features = [
    "Open", "High", "Low", "Close", "Volume",
    "RSI14",    # Relative Strength Index (14-day)
    "MACD"      # Moving Average Convergence Divergence
]
```

**Data Flow**:
1. Raw OHLCV data fetched from Yahoo Finance
2. Technical indicators calculated
3. Data stored in Parquet format
4. Feast materializes features for training/serving
5. Point-in-time joins ensure no future data leakage

**Storage**:
- **Offline Store**: Parquet files (`feature_store/data/features.parquet`)
- **Registry**: SQLite database (`feature_store/data/registry.db`)
- **Entity**: Ticker symbol with timestamp

---

### 3.5 Experiment Tracking: MLflow + DagsHub

**Purpose**: Model versioning, experiment tracking, and artifact storage

**MLflow Components**:

1. **Tracking Server**
   - Logs parameters, metrics, and artifacts
   - Hosted on DagsHub for remote access
   - Automatic experiment organization

2. **Model Registry**
   - Versions all trained models
   - Tracks model lineage
   - Stores model metadata

3. **Artifact Store**
   - Stores model files (.pt)
   - Stores scalers (.pkl)
   - Stores evaluation plots (.png)

**DagsHub Integration**:
```python
import dagshub
import mlflow

# Initialize DagsHub tracking
dagshub.init(repo_owner="username", repo_name="stock-agent-ops", mlflow=True)

# Set tracking URI
mlflow.set_tracking_uri(os.getenv("MLFLOW_TRACKING_URI"))

# Log experiment
with mlflow.start_run(run_name=f"train_{ticker}"):
    mlflow.log_params({"epochs": 20, "batch_size": 32})
    mlflow.log_metrics({"mse": 0.001, "r2": 0.95})
    mlflow.log_artifact("model.pt")
```

**Benefits**:
- **Collaboration**: Team members can view experiments
- **Reproducibility**: All parameters and artifacts tracked
- **Comparison**: Easy experiment comparison
- **Deployment**: Direct model deployment from registry

---

### 3.6 Caching Layer: Redis

**Purpose**: High-speed data caching and state management

**Use Cases**:

1. **Prediction Caching**
   - Key: `predict_child_{ticker}`
   - TTL: 86400 seconds (1 day)
   - Value: JSON prediction data

2. **Rate Limiting**
   - Sliding window algorithm
   - Per-endpoint limits (5/hour for training, 40/hour for predictions)
   - Automatic cleanup

3. **Task Status Tracking**
   - Background training job status
   - Elapsed time calculation
   - Completion timestamps

**Configuration**:
```yaml
redis:
  image: redis/redis-stack:latest
  ports:
    - "6379:6379"  # Redis server
    - "8001:8001"  # RedisInsight UI
  volumes:
    - redis_data:/data
```

**Performance**:
- Average latency: <1ms
- Throughput: 100,000+ ops/sec
- Memory: Configurable (default: 256MB)

---

### 3.7 Vector Database: Qdrant

**Purpose**: Semantic caching and similarity search

**Why Qdrant?**
- **High Performance**: Optimized for vector similarity search
- **Scalability**: Handles millions of vectors
- **Filtering**: Metadata filtering for ticker-specific queries
- **Persistence**: Automatic data persistence

**Schema**:
```python
{
    "vector": [768-dim embedding],  # nomic-embed-text
    "payload": {
        "ticker": "AAPL",
        "summary": "Full report text",
        "recommendation": "BULLISH",
        "confidence": "High",
        "last_price": 175.50,
        "created_at_ts": 1703520000,
        "predictions": {...}
    }
}
```

**Semantic Caching Logic**:
1. User requests analysis for "AAPL"
2. Query embedded: `"Analysis report for AAPL"` ‚Üí 768-dim vector
3. Qdrant searches for similar vectors (cosine similarity)
4. If score > 0.95 and ticker matches ‚Üí Cache HIT
5. Return cached report (saves ~10-15 seconds of LLM inference)

**Configuration**:
```yaml
qdrant:
  image: qdrant/qdrant:latest
  ports:
    - "6333:6333"
  volumes:
    - qdrant_data:/qdrant/storage
```

---

### 3.8 API Framework: FastAPI

**Purpose**: High-performance async API server

**Why FastAPI?**
- **Async Support**: Native async/await for concurrent requests
- **Auto Documentation**: Swagger UI and ReDoc
- **Type Safety**: Pydantic models for validation
- **Performance**: One of the fastest Python frameworks

**Key Features Used**:

1. **Background Tasks**
   - Async training jobs
   - Non-blocking operations

2. **Dependency Injection**
   - Redis client injection
   - Configuration management

3. **Middleware**
   - CORS for frontend
   - Prometheus instrumentation
   - Rate limiting

4. **Response Models**
   - Type-safe responses
   - Automatic validation

**Performance Optimizations**:
- Workers: 4 Uvicorn workers
- Connection pooling for Redis
- Async database operations
- Response caching

---

### 3.9 Monitoring: Prometheus + Grafana

**Purpose**: System observability and alerting

**Prometheus Metrics**:

1. **Application Metrics**
   ```python
   PREDICTION_COUNTER = Counter(
       'predictions_total',
       'Total predictions made',
       ['type']  # parent/child
   )
   
   PREDICTION_LATENCY = Histogram(
       'prediction_latency_seconds',
       'Prediction latency',
       ['type']
   )
   ```

2. **System Metrics**
   - CPU usage (%)
   - Memory usage (MB)
   - Disk usage (%)
   - Redis connection status

3. **Custom Metrics**
   - Training job duration
   - Cache hit rate
   - Agent execution time

**Grafana Dashboards**:
- Real-time prediction metrics
- System resource utilization
- API endpoint performance
- Error rate tracking

---

### 3.10 Drift Detection: Evidently AI

**Purpose**: Data quality and model performance monitoring

**Checks Performed**:

1. **Data Drift**
   - Distribution comparison (training vs. serving)
   - Statistical tests (KS test, Chi-squared)
   - Feature-level drift scores

2. **Model Performance**
   - Prediction accuracy over time
   - Error distribution analysis

3. **Data Quality**
   - Missing values
   - Outlier detection
   - Range validation

**Output**:
- HTML reports with visualizations
- JSON summaries for programmatic access
- Drift scores and health status

---

## 4. Low-Level Design (LLD)

### 4.1 Data Ingestion & Feature Engineering

#### 4.1.1 Architecture

```mermaid
graph LR
    START([Start]) --> FETCH[Fetch OHLCV<br/>Yahoo Finance]
    FETCH --> VALIDATE[Validate Data]
    VALIDATE --> CALC[Calculate Indicators]
    CALC --> FEAST[Store in Feast]
    FEAST --> PARQUET[(Parquet Files)]
    
    style FETCH fill:#e3f2fd
    style CALC fill:#f3e5f5
    style FEAST fill:#e8f5e9
```

#### 4.1.2 Data Source: Yahoo Finance

**API**: `yfinance` Python library

**Data Retrieved**:
```python
{
    "Date": "2024-01-01",
    "Open": 175.20,
    "High": 177.50,
    "Low": 174.80,
    "Close": 176.90,
    "Volume": 52000000,
    "Adj Close": 176.90
}
```

**Date Range**:
- Start: 2004-08-19 (Google's IPO date)
- End: Current date
- Frequency: Daily

#### 4.1.3 Technical Indicators

**RSI (Relative Strength Index)**:
```python
def calculate_rsi(data, period=14):
    delta = data['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi
```

**MACD (Moving Average Convergence Divergence)**:
```python
def calculate_macd(data, fast=12, slow=26, signal=9):
    ema_fast = data['Close'].ewm(span=fast).mean()
    ema_slow = data['Close'].ewm(span=slow).mean()
    macd = ema_fast - ema_slow
    signal_line = macd.ewm(span=signal).mean()
    return macd
```

#### 4.1.4 Feast Integration

**Feature View Definition**:
```python
from feast import FeatureView, Entity, Field
from feast.types import Float32, Int64

stock_entity = Entity(name="ticker", join_keys=["ticker"])

stock_features = FeatureView(
    name="stock_features",
    entities=[stock_entity],
    schema=[
        Field(name="open", dtype=Float32),
        Field(name="high", dtype=Float32),
        Field(name="low", dtype=Float32),
        Field(name="close", dtype=Float32),
        Field(name="volume", dtype=Int64),
        Field(name="rsi14", dtype=Float32),
        Field(name="macd", dtype=Float32),
    ],
    source=ParquetSource(path="data/features.parquet")
)
```

**Materialization**:
```python
# Materialize features for training
feature_store = FeatureStore(repo_path="feature_store")
training_data = feature_store.get_historical_features(
    entity_df=entity_df,
    features=["stock_features:*"]
).to_df()
```

#### 4.1.5 Data Pipeline Flow

```mermaid
sequenceDiagram
    participant API
    participant YF as Yahoo Finance
    participant Calc as Calculator
    participant Feast
    participant Parquet
    
    API->>YF: Request OHLCV data
    YF-->>API: Raw price data
    API->>Calc: Calculate RSI, MACD
    Calc-->>API: Enhanced data
    API->>Feast: Register features
    Feast->>Parquet: Write features
    Parquet-->>Feast: Confirm write
    Feast-->>API: Features ready
```

---

### 4.2 Training Pipeline

#### 4.2.1 Transfer Learning Architecture

```mermaid
graph TB
    subgraph "Parent Training"
        P_DATA[S&P 500 Data<br/>20+ years] --> P_TRAIN[Train LSTM<br/>20 epochs]
        P_TRAIN --> P_MODEL[Parent Model<br/>^GSPC]
    end
    
    subgraph "Child Training"
        C_DATA[Stock Data<br/>AAPL, NVDA, etc.] --> LOAD[Load Parent Weights]
        P_MODEL --> LOAD
        LOAD --> FREEZE{Transfer Strategy}
        FREEZE -->|Freeze| F_TRAIN[Train Dense Layers Only<br/>10 epochs]
        FREEZE -->|Fine-tune| FT_TRAIN[Train All Layers<br/>Lower LR]
        F_TRAIN --> C_MODEL[Child Model<br/>AAPL]
        FT_TRAIN --> C_MODEL
    end
    
    P_MODEL --> MLFLOW[MLflow Registry]
    C_MODEL --> MLFLOW
    
    style P_MODEL fill:#e3f2fd
    style C_MODEL fill:#e8f5e9
    style MLFLOW fill:#fff3e0
```

#### 4.2.2 Model Architecture

**LSTM Network**:
```python
class StockLSTM(nn.Module):
    def __init__(self, input_size=7, hidden_size=128, num_layers=2):
        super().__init__()
        
        # LSTM layers
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.2
        )
        
        # Dense layers
        self.fc1 = nn.Linear(hidden_size, 64)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
        self.fc2 = nn.Linear(64, 1)
    
    def forward(self, x):
        # x shape: (batch, seq_len, features)
        lstm_out, _ = self.lstm(x)
        # Take last timestep
        last_out = lstm_out[:, -1, :]
        # Dense layers
        out = self.fc1(last_out)
        out = self.relu(out)
        out = self.dropout(out)
        out = self.fc2(out)
        return out
```

**Hyperparameters**:
- Input size: 7 features
- Hidden size: 128 neurons
- LSTM layers: 2
- Dropout: 0.2
- Batch size: 32
- Context length: 60 days
- Prediction length: 5-7 days

#### 4.2.3 Training Process

**Parent Model Training**:
```python
def train_parent():
    # 1. Fetch S&P 500 data
    data = fetch_ohlcv("^GSPC", start="2004-08-19")
    
    # 2. Feature engineering
    data = calculate_indicators(data)
    
    # 3. Store in Feast
    feast_store.ingest(data)
    
    # 4. Create sequences
    X, y = create_sequences(data, context_len=60, pred_len=5)
    
    # 5. Train/test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    
    # 6. Initialize model
    model = StockLSTM(input_size=7, hidden_size=128)
    optimizer = Adam(model.parameters(), lr=0.001)
    criterion = MSELoss()
    
    # 7. Training loop
    for epoch in range(20):
        model.train()
        for batch_X, batch_y in dataloader:
            optimizer.zero_grad()
            predictions = model(batch_X)
            loss = criterion(predictions, batch_y)
            loss.backward()
            optimizer.step()
    
    # 8. Save model
    torch.save(model.state_dict(), "outputs/parent/^gspc_parent_model.pt")
    
    # 9. Log to MLflow
    mlflow.log_model(model, "parent_model")
    mlflow.log_metrics({"mse": mse, "r2": r2})
```

**Child Model Training (Transfer Learning)**:
```python
def train_child(ticker, strategy="freeze"):
    # 1. Load parent model
    parent_model = StockLSTM(input_size=7, hidden_size=128)
    parent_model.load_state_dict(torch.load("outputs/parent/^gspc_parent_model.pt"))
    
    # 2. Fetch child data
    data = fetch_ohlcv(ticker, start="2004-08-19")
    data = calculate_indicators(data)
    
    # 3. Create sequences
    X, y = create_sequences(data, context_len=60, pred_len=5)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    
    # 4. Transfer learning strategy
    if strategy == "freeze":
        # Freeze LSTM layers
        for param in parent_model.lstm.parameters():
            param.requires_grad = False
        optimizer = Adam(filter(lambda p: p.requires_grad, parent_model.parameters()), lr=0.001)
    else:  # fine_tune
        # Train all layers with lower learning rate
        optimizer = Adam(parent_model.parameters(), lr=0.0001)
    
    # 5. Fine-tune on child data
    for epoch in range(10):
        parent_model.train()
        for batch_X, batch_y in dataloader:
            optimizer.zero_grad()
            predictions = parent_model(batch_X)
            loss = criterion(predictions, batch_y)
            loss.backward()
            optimizer.step()
    
    # 6. Save child model
    torch.save(parent_model.state_dict(), f"outputs/{ticker.lower()}/{ticker}_child_model.pt")
    
    # 7. Log to MLflow
    mlflow.log_model(parent_model, f"child_model_{ticker}")
```

#### 4.2.4 Training Workflow

```mermaid
sequenceDiagram
    participant API
    participant Redis
    participant Trainer
    participant Feast
    participant Model
    participant MLflow
    
    API->>Redis: Set task status (running)
    API->>Trainer: Start training job
    Trainer->>Feast: Fetch features
    Feast-->>Trainer: Training data
    Trainer->>Model: Initialize/Load model
    
    loop Training Epochs
        Trainer->>Model: Forward pass
        Model-->>Trainer: Predictions
        Trainer->>Model: Backward pass
        Trainer->>Model: Update weights
    end
    
    Trainer->>MLflow: Log metrics & artifacts
    Trainer->>Redis: Update task status (completed)
    Trainer-->>API: Training complete
```

#### 4.2.5 Evaluation Metrics

**Metrics Calculated**:
1. **MSE (Mean Squared Error)**: Average squared difference
2. **RMSE (Root Mean Squared Error)**: Square root of MSE
3. **R¬≤ Score**: Coefficient of determination
4. **MAE (Mean Absolute Error)**: Average absolute difference

**Visualization**:
- Predictions vs. Actuals plot
- Loss curve over epochs
- Residual distribution

---

### 4.3 Inference Pipeline

#### 4.3.1 Architecture

```mermaid
graph TB
    START([API Request]) --> CHECK_CACHE{Redis Cache?}
    CHECK_CACHE -->|HIT| RETURN_CACHE[Return Cached Result]
    CHECK_CACHE -->|MISS| CHECK_MODEL{Model Exists?}
    CHECK_MODEL -->|NO| TRAIN[Trigger Training<br/>Return 202]
    CHECK_MODEL -->|YES| LOAD[Load Model & Scaler]
    LOAD --> FETCH[Fetch Latest Data]
    FETCH --> PREPROCESS[Preprocess & Scale]
    PREPROCESS --> PREDICT[LSTM Inference]
    PREDICT --> POSTPROCESS[Inverse Scale]
    POSTPROCESS --> PLOT[Generate Plot]
    PLOT --> CACHE[Store in Redis]
    CACHE --> RETURN[Return Result]
    
    style CHECK_CACHE fill:#ffebee
    style CHECK_MODEL fill:#e8f5e9
    style PREDICT fill:#e3f2fd
```

#### 4.3.2 Prediction Process

**Prediction Caching**:
```python
cache_key = f"predict_child_{ticker.lower()}"
cached = redis_client.get(cache_key)
if cached:
    return json.loads(cached)
```

2. **Model Loading**:
```python
model_path = f"outputs/{ticker.lower()}/{ticker}_child_model.pt"
scaler_path = f"outputs/{ticker.lower()}/{ticker}_scaler.pkl"

model = StockLSTM(input_size=7, hidden_size=128)
model.load_state_dict(torch.load(model_path))
model.eval()

scaler = joblib.load(scaler_path)
```

3. **Data Preparation**:
```python
# Fetch latest 60 days
data = fetch_ohlcv(ticker, period="3mo")
data = calculate_indicators(data)

# Get last 60 days as context
context = data[-60:][features].values
context_scaled = scaler.transform(context)
context_tensor = torch.FloatTensor(context_scaled).unsqueeze(0)
```

4. **Inference**:
```python
with torch.no_grad():
    predictions = []
    current_context = context_tensor
    
    for _ in range(7):  # 7-day forecast
        pred = model(current_context)
        predictions.append(pred.item())
        
        # Update context (sliding window)
        new_row = torch.cat([current_context[:, 1:, :], pred.unsqueeze(1)], dim=1)
        current_context = new_row
```

5. **Post-processing**:
```python
# Inverse scale predictions
predictions_original = scaler.inverse_transform(predictions)

# Create forecast dates
forecast_dates = pd.date_range(start=data.index[-1] + timedelta(days=1), periods=7)

# Format output
forecast = [
    {"date": str(date), "close": float(price)}
    for date, price in zip(forecast_dates, predictions_original)
]
```

6. **Caching**:
```python
result = {
    "predictions": {
        "historical": historical_data,
        "full_forecast": forecast
    }
}
redis_client.setex(cache_key, 86400, json.dumps(result))  # 1-day TTL
```

#### 4.3.3 Auto-Training Logic (Synchronous Chaining)

**New Mechanism**: When auto-training is triggered, the system uses a **chaining mechanism** (`chain_fn`) to ensure that prediction and caching happen *immediately* after the model training is finished, **within the same background task**.

This ensures that when the client (Agent or User) polls `/status` and sees `completed`, the prediction data is **guaranteed** to be in Redis.

```python
@router.post("/predict-child")
async def predict_child_endpoint(request: Request, response: Response):
    data = await request.json()
    ticker = data.get("ticker", "").strip().upper()
    
    try:
        # 1. Attempt prediction (Check Cache -> Check Model -> Predict)
        def get_preds():
            # Tries to get from Redis, else loads model and predicts
            return get_or_set_cache(f"predict_child_{ticker.lower()}", lambda: predict_child(ticker))
            
        result, _ = await run_blocking_fn(get_preds)
        return {"result": result}
    
    except (FileNotFoundError, PipelineError):
        # 2. Model Missing -> Trigger Training + Chained Prediction
        logger.info(f"Model missing for {ticker}, triggering auto-training.")
        
        task_id = ticker.lower()
        
        # Define the chained task: Predict & Cache immediately after training
        def chain_predict():
            logger.info(f"Auto-predicting for {ticker} after training...")
            # This populates Redis so the next request is a CACHE HIT
            get_or_set_cache(f"predict_child_{ticker.lower()}", lambda: predict_child(ticker), expire=86400)

        # Start the background task with the chain function
        await run_training(task_id, train_child, ticker, chain_fn=chain_predict)
        
        response.status_code = 202
        return {
            "status": "training",
            "detail": f"Model for {ticker} missing. Training started (with auto-prediction).",
            "task_id": task_id
        }
```

---

### 4.4 AI Agent System

#### 4.4.1 Multi-Agent Architecture

```mermaid
graph TB
    START([Start Analysis]) --> CACHE_CHECK{Semantic<br/>Cache?}
    CACHE_CHECK -->|HIT| RETURN_CACHED[Return Cached Report]
    CACHE_CHECK -->|MISS| FETCH_PRED[Fetch Predictions]
    
    FETCH_PRED --> PERF[Performance Analyst]
    PERF --> NEWS[Market Expert]
    NEWS --> REPORT[Report Generator]
    REPORT --> CRITIC[Critic]
    CRITIC --> SAVE[Save to Qdrant]
    SAVE --> RETURN[Return Report]
    
    style CACHE_CHECK fill:#e8f5e9
    style PERF fill:#e3f2fd
    style NEWS fill:#f3e5f5
    style REPORT fill:#fff3e0
    style CRITIC fill:#ffebee
```

#### 4.4.2 Agent Implementations

**1. Performance Analyst**:
```python
def performance_analyst_node(state: dict) -> dict:
    ticker = state["ticker"]
    predictions = state.get("predictions")
    
    prompt = f"""
    You are a quantitative analyst. Analyze these predictions:
    
    {predictions}
    
    Provide:
    1. Trend direction (up/down/sideways)
    2. Volatility assessment
    3. Key price levels
    4. Risk factors
    
    Be concise and data-driven.
    """
    
    response = llm.invoke([SystemMessage(content=prompt)])
    
    return {
        "messages": [response],
        "performance_analysis": response.content
    }
```

**2. Market Expert**:
```python
def market_expert_node(state: dict) -> dict:
    ticker = state["ticker"]
    
    # Fetch news using Yahoo Finance
    news = get_stock_news(ticker)
    
    prompt = f"""
    You are a market strategist. Analyze this news:
    
    {news}
    
    Provide a 3-5 line sentiment summary covering:
    - Overall market sentiment
    - Key catalysts or risks
    - Sector trends
    """
    
    response = llm.invoke([SystemMessage(content=prompt)])
    
    return {
        "messages": [response],
        "news_sentiment": response.content
    }
```

**3. Report Generator**:
```python
def report_generator_node(state: dict) -> dict:
    ticker = state["ticker"]
    predictions = state.get("predictions", "")
    news = state.get("news_sentiment", "")
    
    prompt = f"""
    Write a Bloomberg-style markdown report for {ticker}.
    
    TECHNICAL ANALYSIS:
    {predictions}
    
    MARKET SENTIMENT:
    {news}
    
    Structure:
    # {ticker} Analysis Report
    
    ## Executive Summary
    [2-3 sentences]
    
    ## Technical Outlook
    [Price predictions and trends]
    
    ## Market Sentiment
    [News and sentiment analysis]
    
    ## Recommendation
    **Market Stance:** BULLISH/BEARISH/NEUTRAL
    **Confidence:** High/Medium/Low
    **Rationale:** [1-2 sentences]
    """
    
    response = llm.invoke([SystemMessage(content=prompt)])
    text = response.content
    
    # Extract recommendation
    upper = text.upper()
    stance = "BULLISH" if "BULLISH" in upper else "BEARISH" if "BEARISH" in upper else "NEUTRAL"
    confidence = "High" if "HIGH" in upper else "Low" if "LOW" in upper else "Medium"
    
    return {
        "messages": [response],
        "final_report": text,
        "recommendation": stance,
        "confidence": confidence
    }
```

**4. Critic**:
```python
def critic_node(state: dict) -> dict:
    current_report = state.get("final_report", "")
    predictions = state.get("predictions", "")
    
    prompt = f"""
    You are a Senior Editor reviewing a financial report.
    
    PREDICTIONS:
    {predictions}
    
    DRAFT REPORT:
    {current_report}
    
    Tasks:
    1. Verify recommendation aligns with data
    2. Check for logical consistency
    3. Ensure professional Bloomberg tone
    4. Fix any errors or improve clarity
    
    Output: Final polished report (or original if already perfect)
    """
    
    response = llm.invoke([SystemMessage(content=prompt)])
    
    return {
        "messages": [response],
        "final_report": response.content
    }
```

#### 4.4.3 Semantic Caching with Qdrant

**Embedding Generation**:
```python
# Create query embedding
query_text = f"Analysis report for {ticker}"
query_vector = embedder.embed_query(query_text)  # 768-dim vector
```

**Cache Lookup**:
```python
# Search Qdrant
hits = qdrant_client.search(
    collection_name="dataset_cache",
    query_vector=query_vector,
    limit=5,
    query_filter={
        "must": [
            {"key": "ticker", "match": {"value": ticker}}
        ]
    }
)

# Check for high similarity
valid_hits = [h for h in hits if h.score > 0.95]

if valid_hits:
    # Sort by timestamp (newest first)
    valid_hits.sort(key=lambda x: x.payload.get("created_at_ts", 0), reverse=True)
    best_hit = valid_hits[0]
    
    # Return cached report
    return {
        "final_report": best_hit.payload.get("summary"),
        "recommendation": best_hit.payload.get("recommendation"),
        "confidence": best_hit.payload.get("confidence"),
        "predictions": best_hit.payload.get("predictions")
    }
```

**Cache Storage**:
```python
# Save new report
qdrant_client.upsert(
    collection_name="dataset_cache",
    points=[
        {
            "id": str(uuid.uuid4()),
            "vector": query_vector,
            "payload": {
                "ticker": ticker,
                "summary": final_report,
                "recommendation": recommendation,
                "confidence": confidence,
                "last_price": last_price,
                "created_at_ts": int(time.time()),
                "predictions": predictions_dict
            }
        }
    ]
)
```

#### 4.4.4 Agent Execution Flow

```mermaid
sequenceDiagram
    participant API
    participant Cache as Qdrant
    participant Embed as Ollama Embeddings
    participant Graph as LangGraph
    participant LLM as Ollama LLM
    
    API->>Embed: Embed query
    Embed-->>API: Query vector
    API->>Cache: Search similar reports
    
    alt Cache Hit (score > 0.95)
        Cache-->>API: Cached report
    else Cache Miss
        API->>Graph: Start agent workflow
        Graph->>LLM: Performance Analyst
        LLM-->>Graph: Analysis
        Graph->>LLM: Market Expert
        LLM-->>Graph: Sentiment
        Graph->>LLM: Report Generator
        LLM-->>Graph: Draft report
        Graph->>LLM: Critic
        LLM-->>Graph: Final report
        Graph-->>API: Complete analysis
        API->>Cache: Store report + embedding
    end
    
    API-->>API: Return to user
```

---

### 4.5 Monitoring & Observability

#### 4.5.1 Metrics Collection

```mermaid
graph LR
    API[FastAPI] --> PROM[Prometheus]
    API --> PSUTIL[psutil]
    PSUTIL --> SYS_MET[System Metrics]
    SYS_MET --> PROM
    PROM --> GRAF[Grafana]
    
    API --> EVID[Evidently AI]
    EVID --> DRIFT[Drift Reports]
    
    style PROM fill:#e3f2fd
    style GRAF fill:#e8f5e9
    style EVID fill:#fff3e0
```

#### 4.5.2 Prometheus Metrics

**Application Metrics**:
```python
from prometheus_client import Counter, Histogram, Gauge

# Prediction counters
PREDICTION_COUNTER = Counter(
    'predictions_total',
    'Total number of predictions made',
    ['type']  # parent or child
)

# Prediction latency
PREDICTION_LATENCY = Histogram(
    'prediction_latency_seconds',
    'Time taken for predictions',
    ['type'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
)

# System metrics
CPU_USAGE = Gauge('system_cpu_percent', 'CPU usage percentage')
MEMORY_USAGE = Gauge('system_memory_mb', 'Memory usage in MB')
DISK_USAGE = Gauge('system_disk_percent', 'Disk usage percentage')
REDIS_STATUS = Gauge('redis_connection_status', 'Redis connection status')
```

**Metric Collection**:
```python
def refresh_system_metrics():
    CPU_USAGE.set(psutil.cpu_percent())
    MEMORY_USAGE.set(psutil.virtual_memory().used / 1024 / 1024)
    DISK_USAGE.set(psutil.disk_usage('/').percent)
```

**Instrumentation**:
```python
@router.post("/predict-child")
async def predict_child_endpoint(request: Request):
    PREDICTION_COUNTER.labels(type="child").inc()
    
    start_time = time.time()
    try:
        result = await run_blocking_fn(predict_child, ticker)
        PREDICTION_LATENCY.labels(type="child").observe(time.time() - start_time)
        return {"result": result}
    except Exception as e:
        raise HTTPException(500, str(e))
```

#### 4.5.3 Drift Detection

**Evidently AI Integration**:
```python
from evidently import ColumnMapping
from evidently.report import Report
from evidently.metric_preset import DataDriftPreset

def check_drift(ticker, output_dir):
    # Load reference data (training)
    reference_data = load_training_data(ticker)
    
    # Load current data (serving)
    current_data = load_serving_data(ticker)
    
    # Define column mapping
    column_mapping = ColumnMapping(
        numerical_features=["Open", "High", "Low", "Close", "Volume", "RSI14", "MACD"]
    )
    
    # Generate drift report
    report = Report(metrics=[DataDriftPreset()])
    report.run(
        reference_data=reference_data,
        current_data=current_data,
        column_mapping=column_mapping
    )
    
    # Save HTML report
    report.save_html(f"{output_dir}/{ticker}/drift/drift_report.html")
    
    # Extract metrics
    drift_score = report.as_dict()["metrics"][0]["result"]["drift_score"]
    
    return {
        "drift_score": drift_score,
        "health": "Healthy" if drift_score < 0.3 else "Degraded"
    }
```

#### 4.5.4 Agent Evaluation

**Evaluation Criteria**:
```python
class AgentEvaluator:
    def evaluate_live(self, ticker):
        # Load latest agent output
        output_path = f"outputs/{ticker}/agent_eval/latest_output.json"
        with open(output_path, 'r') as f:
            output = json.load(f)
        
        # Run checks
        checks = {
            "has_recommendation": bool(output.get("recommendation")),
            "has_confidence": bool(output.get("confidence")),
            "has_report": len(output.get("final_report", "")) > 100,
            "valid_stance": output.get("recommendation") in ["BULLISH", "BEARISH", "NEUTRAL"],
            "valid_confidence": output.get("confidence") in ["High", "Medium", "Low"]
        }
        
        # Calculate score
        score = sum(checks.values()) / len(checks)
        
        return {
            "metrics": {
                "overall_score": score,
                "checks": checks,
                "status": "Pass" if score >= 0.8 else "Fail"
            }
        }
```

---

## 5. Infrastructure & Deployment

### 5.1 Docker Compose (Local Development)

**Services**:
1. **FastAPI** - Main API server
2. **Redis** - Caching and state management
3. **Qdrant** - Vector database
4. **Prometheus** - Metrics collection
5. **Grafana** - Visualization
6. **Frontend** - Streamlit UI
7. **Monitoring Dashboard** - Streamlit monitoring app

**Network**:
- Bridge network for inter-service communication
- Port mapping for external access

**Volumes**:
- Persistent storage for Redis and Qdrant
- Shared volumes for outputs and MLflow artifacts

### 5.2 Kubernetes (Production)

**Architecture**:
```mermaid
graph TB
    subgraph "Ingress Layer"
        ING[Ingress Controller]
    end
    
    subgraph "Application Layer"
        API_POD[FastAPI Pods<br/>Replicas: 3]
        UI_POD[Frontend Pods<br/>Replicas: 2]
    end
    
    subgraph "Data Layer"
        REDIS_POD[Redis StatefulSet]
        QDRANT_POD[Qdrant StatefulSet]
    end
    
    subgraph "Monitoring Layer"
        PROM_POD[Prometheus Pod]
        GRAF_POD[Grafana Pod]
    end
    
    ING --> API_POD
    ING --> UI_POD
    API_POD --> REDIS_POD
    API_POD --> QDRANT_POD
    API_POD --> PROM_POD
    PROM_POD --> GRAF_POD
```

**Deployment Strategy**:
- Rolling updates with zero downtime
- Health checks and readiness probes
- Horizontal Pod Autoscaling (HPA)
- Resource limits and requests

### 5.3 CI/CD Pipeline

**GitHub Actions Workflow**:
```yaml
name: CI/CD Pipeline

on:
  push:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Run tests
        run: pytest tests/
  
  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Build Docker images
        run: docker-compose build
      
      - name: Push to ECR
        run: |
          aws ecr get-login-password | docker login --username AWS --password-stdin $ECR_REGISTRY
          docker push $ECR_REGISTRY/fastapi:latest
  
  deploy:
    needs: build
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to EKS
        run: |
          kubectl set image deployment/fastapi fastapi=$ECR_REGISTRY/fastapi:latest
          kubectl rollout status deployment/fastapi
```

---

## 6. API Reference

### 6.1 Core Endpoints

| Endpoint | Method | Description | Rate Limit |
|----------|--------|-------------|------------|
| `/` | GET | Project information | None |
| `/health` | GET | Health check | None |
| `/train-parent` | POST | Train parent model | 5/hour |
| `/train-child` | POST | Train child model | 5/hour |
| `/predict-parent` | POST | Parent predictions | 40/hour |
| `/predict-child` | POST | Child predictions | 40/hour |
| `/analyze` | POST | Full AI analysis | 40/hour |
| `/status/{task_id}` | GET | Training status | None |
| `/outputs` | GET | List outputs | None |
| `/system/reset` | DELETE | Reset system | None |
| `/metrics` | GET | Prometheus metrics | None |

### 6.2 Request/Response Examples

**Prediction Request**:
```bash
curl -X POST http://localhost:8000/predict-child \
  -H "Content-Type: application/json" \
  -d '{"ticker": "AAPL"}'
```

**Response**:
```json
{
  "result": {
    "predictions": {
      "historical": [
        {"date": "2024-01-01", "close": 175.20},
        {"date": "2024-01-02", "close": 176.50}
      ],
      "full_forecast": [
        {"date": "2024-01-08", "close": 177.80},
        {"date": "2024-01-09", "close": 178.20},
        {"date": "2024-01-10", "close": 179.10}
      ]
    }
  }
}
```

---

## 7. Performance & Scalability

### 7.1 Performance Metrics

| Operation | Latency | Throughput |
|-----------|---------|------------|
| Prediction (cached) | <10ms | 10,000 req/s |
| Prediction (uncached) | 200-500ms | 100 req/s |
| Training (child) | 2-5 min | N/A |
| Training (parent) | 10-15 min | N/A |
| Agent analysis (cached) | <50ms | 1,000 req/s |
| Agent analysis (uncached) | 10-15s | 10 req/s |

### 7.2 Scalability Considerations

**Horizontal Scaling**:
- FastAPI workers: 4 per instance
- Kubernetes replicas: 3-10 (auto-scaled)
- Redis cluster mode for high availability

**Vertical Scaling**:
- GPU support for faster training
- Increased memory for larger models
- SSD storage for faster I/O

**Optimization Techniques**:
- Async operations throughout
- Connection pooling
- Batch predictions
- Model quantization

---

## 8. Security & Best Practices

### 8.1 Security Measures

1. **API Security**
   - Rate limiting per endpoint
   - Input validation with Pydantic
   - CORS configuration

2. **Data Security**
   - Environment variables for secrets
   - No hardcoded credentials
   - Encrypted connections

3. **Infrastructure Security**
   - Network isolation
   - Resource limits
   - Health checks

### 8.2 Best Practices

1. **Code Quality**
   - Type hints throughout
   - Comprehensive logging
   - Error handling

2. **MLOps**
   - Experiment tracking
   - Model versioning
   - Reproducible training

3. **Monitoring**
   - Real-time metrics
   - Alerting rules
   - Drift detection

---

## Appendix A: Configuration Reference

### Environment Variables

```bash
# DagsHub/MLflow
DAGSHUB_USER_NAME=your_username
DAGSHUB_REPO_NAME=stock-agent-ops
DAGSHUB_TOKEN=your_token
MLFLOW_TRACKING_URI=https://dagshub.com/...

# Redis
REDIS_HOST=redis
REDIS_PORT=6379

# Ollama
OLLAMA_BASE_URL=http://host.docker.internal:11434

# APIs
GOOGLE_API_KEY=your_key
LANGSMITH_API_KEY=your_key
```

### Model Hyperparameters

```python
{
    "context_len": 60,
    "pred_len": 5,
    "batch_size": 32,
    "parent_epochs": 20,
    "child_epochs": 10,
    "hidden_size": 128,
    "num_layers": 2,
    "dropout": 0.2,
    "learning_rate": 0.001,
    "fine_tune_lr": 0.0001
}
```

---

## Appendix B: Troubleshooting Guide

### Common Issues

1. **Model Not Found**
   - Solution: System auto-trains, wait for completion

2. **Redis Connection Failed**
   - Solution: Check Redis container status, restart if needed

3. **Ollama Not Responding**
   - Solution: Ensure Ollama is running on host machine

4. **Out of Memory**
   - Solution: Reduce batch size or context length

---

## Appendix C: Future Enhancements

1. **Real-time Streaming**
   - WebSocket support for live predictions
   - Streaming LLM responses

2. **Advanced Models**
   - Transformer-based architectures
   - Ensemble methods

3. **Additional Data Sources**
   - Alternative data (sentiment, social media)
   - Fundamental data (earnings, ratios)

4. **Enhanced Monitoring**
   - Anomaly detection
   - Automated retraining triggers

---

**Document Version**: 1.0  
**Last Updated**: December 25, 2025  
**Authors**: MLOps Team  
**Status**: Production Ready

---

*This documentation is a living document and will be updated as the system evolves.*
