{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e081591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495d6309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Training parent model for S&P 500...\n",
      "Fetched 5288 rows for ^GSPC\n",
      "Epoch 1/20 - Train Loss: 0.40425\n",
      "Epoch 1/20 - Val Loss: 0.18780\n",
      "Epoch 2/20 - Train Loss: 0.17264\n",
      "Epoch 2/20 - Val Loss: 0.14371\n",
      "Epoch 3/20 - Train Loss: 0.14790\n",
      "Epoch 3/20 - Val Loss: 0.13367\n",
      "Epoch 4/20 - Train Loss: 0.13877\n",
      "Epoch 4/20 - Val Loss: 0.12930\n",
      "Epoch 5/20 - Train Loss: 0.13355\n",
      "Epoch 5/20 - Val Loss: 0.12181\n",
      "Epoch 6/20 - Train Loss: 0.12832\n",
      "Epoch 6/20 - Val Loss: 0.12780\n",
      "Epoch 7/20 - Train Loss: 0.12615\n",
      "Epoch 7/20 - Val Loss: 0.11840\n",
      "Epoch 8/20 - Train Loss: 0.12268\n",
      "Epoch 8/20 - Val Loss: 0.11871\n",
      "Epoch 9/20 - Train Loss: 0.12092\n",
      "Epoch 9/20 - Val Loss: 0.11420\n",
      "Epoch 10/20 - Train Loss: 0.11745\n",
      "Epoch 10/20 - Val Loss: 0.11444\n",
      "Epoch 11/20 - Train Loss: 0.11487\n",
      "Epoch 11/20 - Val Loss: 0.11293\n",
      "Epoch 12/20 - Train Loss: 0.11410\n",
      "Epoch 12/20 - Val Loss: 0.10988\n",
      "Epoch 13/20 - Train Loss: 0.11201\n",
      "Epoch 13/20 - Val Loss: 0.10979\n",
      "Epoch 14/20 - Train Loss: 0.11045\n",
      "Epoch 14/20 - Val Loss: 0.10869\n",
      "Epoch 15/20 - Train Loss: 0.10853\n",
      "Epoch 15/20 - Val Loss: 0.10582\n",
      "Epoch 16/20 - Train Loss: 0.10685\n",
      "Epoch 16/20 - Val Loss: 0.11241\n",
      "Epoch 17/20 - Train Loss: 0.10648\n",
      "Epoch 17/20 - Val Loss: 0.10610\n",
      "Epoch 18/20 - Train Loss: 0.10637\n",
      "Epoch 18/20 - Val Loss: 0.10661\n",
      "Epoch 19/20 - Train Loss: 0.10454\n",
      "Epoch 19/20 - Val Loss: 0.10288\n",
      "Epoch 20/20 - Train Loss: 0.10213\n",
      "Epoch 20/20 - Val Loss: 0.10238\n",
      "Model and scaler saved locally at outputs/parent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lz/6w4cqrgn0cx_hw5sylb99fxh0000gn/T/ipykernel_1474/2361490311.py:215: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(**export_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSPC → MSE: 0.06793, RMSE: 0.26064, R²: 0.92977\n",
      "✓ Parent model trained and saved to: outputs/parent\n",
      "\n",
      "2. Training child models sequentially...\n",
      "Training child model for GOOG...\n",
      "Fetched 5288 rows for GOOG\n",
      "Epoch 1/10 - Train Loss: 0.13160\n",
      "Epoch 1/10 - Val Loss: 0.10523\n",
      "Epoch 2/10 - Train Loss: 0.12313\n",
      "Epoch 2/10 - Val Loss: 0.10268\n",
      "Epoch 3/10 - Train Loss: 0.12049\n",
      "Epoch 3/10 - Val Loss: 0.10066\n",
      "Epoch 4/10 - Train Loss: 0.11911\n",
      "Epoch 4/10 - Val Loss: 0.09937\n",
      "Epoch 5/10 - Train Loss: 0.11801\n",
      "Epoch 5/10 - Val Loss: 0.09905\n",
      "Epoch 6/10 - Train Loss: 0.11629\n",
      "Epoch 6/10 - Val Loss: 0.09803\n",
      "Epoch 7/10 - Train Loss: 0.11651\n",
      "Epoch 7/10 - Val Loss: 0.09756\n",
      "Epoch 8/10 - Train Loss: 0.11612\n",
      "Epoch 8/10 - Val Loss: 0.09695\n",
      "Epoch 9/10 - Train Loss: 0.11442\n",
      "Epoch 9/10 - Val Loss: 0.09663\n",
      "Epoch 10/10 - Train Loss: 0.11430\n",
      "Epoch 10/10 - Val Loss: 0.09611\n",
      "Model and scaler saved locally at outputs/GOOG\n",
      "Input shape for GOOG: (1, 60, 7)\n",
      "Prediction shape for GOOG: (1, 5, 7)\n",
      "Inverse transformed shape for GOOG: (5, 7)\n",
      "Forecast entry 0 for GOOG: {'date': '2025-09-15', 'open': 188.99, 'high': 193.69, 'low': 188.54, 'close': 190.51, 'volume': 104224256}\n",
      "Forecast entry 1 for GOOG: {'date': '2025-09-16', 'open': 189.0, 'high': 191.89, 'low': 187.64, 'close': 190.63, 'volume': 74471056}\n",
      "Forecast entry 2 for GOOG: {'date': '2025-09-17', 'open': 189.69, 'high': 192.71, 'low': 188.88, 'close': 189.86, 'volume': 70436800}\n",
      "Forecast entry 3 for GOOG: {'date': '2025-09-18', 'open': 190.41, 'high': 190.27, 'low': 187.7, 'close': 190.25, 'volume': 64933728}\n",
      "Forecast entry 4 for GOOG: {'date': '2025-09-19', 'open': 189.8, 'high': 192.29, 'low': 188.4, 'close': 189.59, 'volume': 59006620}\n",
      "Prediction output for GOOG: {\n",
      "  \"ticker\": \"GOOG\",\n",
      "  \"last_date\": \"2025-09-12\",\n",
      "  \"future_window_days\": 5,\n",
      "  \"next_business_days\": [\n",
      "    \"2025-09-15\",\n",
      "    \"2025-09-16\",\n",
      "    \"2025-09-17\",\n",
      "    \"2025-09-18\",\n",
      "    \"2025-09-19\"\n",
      "  ],\n",
      "  \"predictions\": {\n",
      "    \"next_day\": {\n",
      "      \"open\": 188.99,\n",
      "      \"high\": 193.69,\n",
      "      \"low\": 188.54,\n",
      "      \"close\": 190.51,\n",
      "      \"volume\": 104224256\n",
      "    },\n",
      "    \"next_week\": {\n",
      "      \"high\": 193.69,\n",
      "      \"low\": 187.64\n",
      "    },\n",
      "    \"full_forecast\": [\n",
      "      {\n",
      "        \"date\": \"2025-09-15\",\n",
      "        \"open\": 188.99,\n",
      "        \"high\": 193.69,\n",
      "        \"low\": 188.54,\n",
      "        \"close\": 190.51,\n",
      "        \"volume\": 104224256\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-16\",\n",
      "        \"open\": 189.0,\n",
      "        \"high\": 191.89,\n",
      "        \"low\": 187.64,\n",
      "        \"close\": 190.63,\n",
      "        \"volume\": 74471056\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-17\",\n",
      "        \"open\": 189.69,\n",
      "        \"high\": 192.71,\n",
      "        \"low\": 188.88,\n",
      "        \"close\": 189.86,\n",
      "        \"volume\": 70436800\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-18\",\n",
      "        \"open\": 190.41,\n",
      "        \"high\": 190.27,\n",
      "        \"low\": 187.7,\n",
      "        \"close\": 190.25,\n",
      "        \"volume\": 64933728\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-19\",\n",
      "        \"open\": 189.8,\n",
      "        \"high\": 192.29,\n",
      "        \"low\": 188.4,\n",
      "        \"close\": 189.59,\n",
      "        \"volume\": 59006620\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "Forecast closes for GOOG: [190.51, 190.63, 189.86, 190.25, 189.59]\n",
      "Last historical close for GOOG: 241.3800048828125\n",
      "Y-values for plotting GOOG: [241.3800048828125, 190.51, 190.63, 189.86, 190.25, 189.59]\n",
      "Plot saved for GOOG at outputs/GOOG/GOOG_child_history_forecast.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lz/6w4cqrgn0cx_hw5sylb99fxh0000gn/T/ipykernel_1474/2361490311.py:215: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(**export_kwargs)\n",
      "/var/folders/lz/6w4cqrgn0cx_hw5sylb99fxh0000gn/T/ipykernel_1474/2361490311.py:397: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  last_close = float(last_close.item() if isinstance(last_close, np.ndarray) else last_close[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOG → MSE: 0.05833, RMSE: 0.24152, R²: 0.93689\n",
      "✓ GOOG model trained and saved to: outputs/GOOG\n",
      "✓ Predictions saved to: outputs/GOOG/GOOG_child_forecast.json\n",
      "✓ Metrics saved to: outputs/GOOG/GOOG_child_metrics.json\n",
      "Training child model for AMZN...\n",
      "Fetched 5288 rows for AMZN\n",
      "Epoch 1/10 - Train Loss: 0.18161\n",
      "Epoch 1/10 - Val Loss: 0.20865\n",
      "Epoch 2/10 - Train Loss: 0.17617\n",
      "Epoch 2/10 - Val Loss: 0.20591\n",
      "Epoch 3/10 - Train Loss: 0.17232\n",
      "Epoch 3/10 - Val Loss: 0.20437\n",
      "Epoch 4/10 - Train Loss: 0.17108\n",
      "Epoch 4/10 - Val Loss: 0.20337\n",
      "Epoch 5/10 - Train Loss: 0.17037\n",
      "Epoch 5/10 - Val Loss: 0.20251\n",
      "Epoch 6/10 - Train Loss: 0.16876\n",
      "Epoch 6/10 - Val Loss: 0.20193\n",
      "Epoch 7/10 - Train Loss: 0.16830\n",
      "Epoch 7/10 - Val Loss: 0.20136\n",
      "Epoch 8/10 - Train Loss: 0.16867\n",
      "Epoch 8/10 - Val Loss: 0.20080\n",
      "Epoch 9/10 - Train Loss: 0.16714\n",
      "Epoch 9/10 - Val Loss: 0.20055\n",
      "Epoch 10/10 - Train Loss: 0.16744\n",
      "Epoch 10/10 - Val Loss: 0.20008\n",
      "Model and scaler saved locally at outputs/AMZN\n",
      "Input shape for AMZN: (1, 60, 7)\n",
      "Prediction shape for AMZN: (1, 5, 7)\n",
      "Inverse transformed shape for AMZN: (5, 7)\n",
      "Forecast entry 0 for AMZN: {'date': '2025-09-15', 'open': 221.19, 'high': 224.61, 'low': 218.14, 'close': 221.15, 'volume': 40364460}\n",
      "Forecast entry 1 for AMZN: {'date': '2025-09-16', 'open': 221.34, 'high': 223.29, 'low': 218.16, 'close': 221.92, 'volume': 43643056}\n",
      "Forecast entry 2 for AMZN: {'date': '2025-09-17', 'open': 220.81, 'high': 225.06, 'low': 218.6, 'close': 220.9, 'volume': 46468348}\n",
      "Forecast entry 3 for AMZN: {'date': '2025-09-18', 'open': 222.1, 'high': 224.1, 'low': 217.59, 'close': 221.86, 'volume': 48109220}\n",
      "Forecast entry 4 for AMZN: {'date': '2025-09-19', 'open': 221.83, 'high': 222.63, 'low': 217.64, 'close': 220.58, 'volume': 46532400}\n",
      "Prediction output for AMZN: {\n",
      "  \"ticker\": \"AMZN\",\n",
      "  \"last_date\": \"2025-09-12\",\n",
      "  \"future_window_days\": 5,\n",
      "  \"next_business_days\": [\n",
      "    \"2025-09-15\",\n",
      "    \"2025-09-16\",\n",
      "    \"2025-09-17\",\n",
      "    \"2025-09-18\",\n",
      "    \"2025-09-19\"\n",
      "  ],\n",
      "  \"predictions\": {\n",
      "    \"next_day\": {\n",
      "      \"open\": 221.19,\n",
      "      \"high\": 224.61,\n",
      "      \"low\": 218.14,\n",
      "      \"close\": 221.15,\n",
      "      \"volume\": 40364460\n",
      "    },\n",
      "    \"next_week\": {\n",
      "      \"high\": 225.06,\n",
      "      \"low\": 217.59\n",
      "    },\n",
      "    \"full_forecast\": [\n",
      "      {\n",
      "        \"date\": \"2025-09-15\",\n",
      "        \"open\": 221.19,\n",
      "        \"high\": 224.61,\n",
      "        \"low\": 218.14,\n",
      "        \"close\": 221.15,\n",
      "        \"volume\": 40364460\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-16\",\n",
      "        \"open\": 221.34,\n",
      "        \"high\": 223.29,\n",
      "        \"low\": 218.16,\n",
      "        \"close\": 221.92,\n",
      "        \"volume\": 43643056\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-17\",\n",
      "        \"open\": 220.81,\n",
      "        \"high\": 225.06,\n",
      "        \"low\": 218.6,\n",
      "        \"close\": 220.9,\n",
      "        \"volume\": 46468348\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-18\",\n",
      "        \"open\": 222.1,\n",
      "        \"high\": 224.1,\n",
      "        \"low\": 217.59,\n",
      "        \"close\": 221.86,\n",
      "        \"volume\": 48109220\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-19\",\n",
      "        \"open\": 221.83,\n",
      "        \"high\": 222.63,\n",
      "        \"low\": 217.64,\n",
      "        \"close\": 220.58,\n",
      "        \"volume\": 46532400\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "Forecast closes for AMZN: [221.15, 221.92, 220.9, 221.86, 220.58]\n",
      "Last historical close for AMZN: 228.14999389648438\n",
      "Y-values for plotting AMZN: [228.14999389648438, 221.15, 221.92, 220.9, 221.86, 220.58]\n",
      "Plot saved for AMZN at outputs/AMZN/AMZN_child_history_forecast.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lz/6w4cqrgn0cx_hw5sylb99fxh0000gn/T/ipykernel_1474/2361490311.py:215: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(**export_kwargs)\n",
      "/var/folders/lz/6w4cqrgn0cx_hw5sylb99fxh0000gn/T/ipykernel_1474/2361490311.py:397: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  last_close = float(last_close.item() if isinstance(last_close, np.ndarray) else last_close[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMZN → MSE: 0.14490, RMSE: 0.38065, R²: 0.85428\n",
      "✓ AMZN model trained and saved to: outputs/AMZN\n",
      "✓ Predictions saved to: outputs/AMZN/AMZN_child_forecast.json\n",
      "✓ Metrics saved to: outputs/AMZN/AMZN_child_metrics.json\n",
      "Training child model for META...\n",
      "Fetched 3336 rows for META\n",
      "Epoch 1/10 - Train Loss: 0.16689\n",
      "Epoch 1/10 - Val Loss: 0.16882\n",
      "Epoch 2/10 - Train Loss: 0.15851\n",
      "Epoch 2/10 - Val Loss: 0.16527\n",
      "Epoch 3/10 - Train Loss: 0.15662\n",
      "Epoch 3/10 - Val Loss: 0.16329\n",
      "Epoch 4/10 - Train Loss: 0.15535\n",
      "Epoch 4/10 - Val Loss: 0.16189\n",
      "Epoch 5/10 - Train Loss: 0.15445\n",
      "Epoch 5/10 - Val Loss: 0.16101\n",
      "Epoch 6/10 - Train Loss: 0.15363\n",
      "Epoch 6/10 - Val Loss: 0.16020\n",
      "Epoch 7/10 - Train Loss: 0.15269\n",
      "Epoch 7/10 - Val Loss: 0.15945\n",
      "Epoch 8/10 - Train Loss: 0.15201\n",
      "Epoch 8/10 - Val Loss: 0.15903\n",
      "Epoch 9/10 - Train Loss: 0.15288\n",
      "Epoch 9/10 - Val Loss: 0.15849\n",
      "Epoch 10/10 - Train Loss: 0.15130\n",
      "Epoch 10/10 - Val Loss: 0.15835\n",
      "Model and scaler saved locally at outputs/META\n",
      "Input shape for META: (1, 60, 7)\n",
      "Prediction shape for META: (1, 5, 7)\n",
      "Inverse transformed shape for META: (5, 7)\n",
      "Forecast entry 0 for META: {'date': '2025-09-15', 'open': 685.84, 'high': 694.6, 'low': 677.42, 'close': 689.82, 'volume': 17689582}\n",
      "Forecast entry 1 for META: {'date': '2025-09-16', 'open': 691.95, 'high': 696.01, 'low': 680.76, 'close': 692.31, 'volume': 17389508}\n",
      "Forecast entry 2 for META: {'date': '2025-09-17', 'open': 687.53, 'high': 699.73, 'low': 678.2, 'close': 689.07, 'volume': 18255380}\n",
      "Forecast entry 3 for META: {'date': '2025-09-18', 'open': 692.86, 'high': 705.45, 'low': 684.07, 'close': 693.82, 'volume': 18800114}\n",
      "Forecast entry 4 for META: {'date': '2025-09-19', 'open': 698.73, 'high': 703.92, 'low': 683.11, 'close': 696.67, 'volume': 18400848}\n",
      "Prediction output for META: {\n",
      "  \"ticker\": \"META\",\n",
      "  \"last_date\": \"2025-09-12\",\n",
      "  \"future_window_days\": 5,\n",
      "  \"next_business_days\": [\n",
      "    \"2025-09-15\",\n",
      "    \"2025-09-16\",\n",
      "    \"2025-09-17\",\n",
      "    \"2025-09-18\",\n",
      "    \"2025-09-19\"\n",
      "  ],\n",
      "  \"predictions\": {\n",
      "    \"next_day\": {\n",
      "      \"open\": 685.84,\n",
      "      \"high\": 694.6,\n",
      "      \"low\": 677.42,\n",
      "      \"close\": 689.82,\n",
      "      \"volume\": 17689582\n",
      "    },\n",
      "    \"next_week\": {\n",
      "      \"high\": 705.45,\n",
      "      \"low\": 677.42\n",
      "    },\n",
      "    \"full_forecast\": [\n",
      "      {\n",
      "        \"date\": \"2025-09-15\",\n",
      "        \"open\": 685.84,\n",
      "        \"high\": 694.6,\n",
      "        \"low\": 677.42,\n",
      "        \"close\": 689.82,\n",
      "        \"volume\": 17689582\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-16\",\n",
      "        \"open\": 691.95,\n",
      "        \"high\": 696.01,\n",
      "        \"low\": 680.76,\n",
      "        \"close\": 692.31,\n",
      "        \"volume\": 17389508\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-17\",\n",
      "        \"open\": 687.53,\n",
      "        \"high\": 699.73,\n",
      "        \"low\": 678.2,\n",
      "        \"close\": 689.07,\n",
      "        \"volume\": 18255380\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-18\",\n",
      "        \"open\": 692.86,\n",
      "        \"high\": 705.45,\n",
      "        \"low\": 684.07,\n",
      "        \"close\": 693.82,\n",
      "        \"volume\": 18800114\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-19\",\n",
      "        \"open\": 698.73,\n",
      "        \"high\": 703.92,\n",
      "        \"low\": 683.11,\n",
      "        \"close\": 696.67,\n",
      "        \"volume\": 18400848\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "Forecast closes for META: [689.82, 692.31, 689.07, 693.82, 696.67]\n",
      "Last historical close for META: 755.5900268554688\n",
      "Y-values for plotting META: [755.5900268554688, 689.82, 692.31, 689.07, 693.82, 696.67]\n",
      "Plot saved for META at outputs/META/META_child_history_forecast.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lz/6w4cqrgn0cx_hw5sylb99fxh0000gn/T/ipykernel_1474/2361490311.py:215: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(**export_kwargs)\n",
      "/var/folders/lz/6w4cqrgn0cx_hw5sylb99fxh0000gn/T/ipykernel_1474/2361490311.py:397: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  last_close = float(last_close.item() if isinstance(last_close, np.ndarray) else last_close[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "META → MSE: 0.11548, RMSE: 0.33982, R²: 0.88289\n",
      "✓ META model trained and saved to: outputs/META\n",
      "✓ Predictions saved to: outputs/META/META_child_forecast.json\n",
      "✓ Metrics saved to: outputs/META/META_child_metrics.json\n",
      "Training child model for AXP...\n",
      "Fetched 5288 rows for AXP\n",
      "Epoch 1/10 - Train Loss: 0.14368\n",
      "Epoch 1/10 - Val Loss: 0.13031\n",
      "Epoch 2/10 - Train Loss: 0.13508\n",
      "Epoch 2/10 - Val Loss: 0.12754\n",
      "Epoch 3/10 - Train Loss: 0.13196\n",
      "Epoch 3/10 - Val Loss: 0.12617\n",
      "Epoch 4/10 - Train Loss: 0.12995\n",
      "Epoch 4/10 - Val Loss: 0.12528\n",
      "Epoch 5/10 - Train Loss: 0.12994\n",
      "Epoch 5/10 - Val Loss: 0.12441\n",
      "Epoch 6/10 - Train Loss: 0.12866\n",
      "Epoch 6/10 - Val Loss: 0.12407\n",
      "Epoch 7/10 - Train Loss: 0.12821\n",
      "Epoch 7/10 - Val Loss: 0.12345\n",
      "Epoch 8/10 - Train Loss: 0.12727\n",
      "Epoch 8/10 - Val Loss: 0.12312\n",
      "Epoch 9/10 - Train Loss: 0.12743\n",
      "Epoch 9/10 - Val Loss: 0.12296\n",
      "Epoch 10/10 - Train Loss: 0.12638\n",
      "Epoch 10/10 - Val Loss: 0.12284\n",
      "Model and scaler saved locally at outputs/AXP\n",
      "Input shape for AXP: (1, 60, 7)\n",
      "Prediction shape for AXP: (1, 5, 7)\n",
      "Inverse transformed shape for AXP: (5, 7)\n",
      "Forecast entry 0 for AXP: {'date': '2025-09-15', 'open': 300.74, 'high': 303.72, 'low': 298.17, 'close': 300.34, 'volume': 2454872}\n",
      "Forecast entry 1 for AXP: {'date': '2025-09-16', 'open': 301.27, 'high': 303.59, 'low': 298.62, 'close': 302.24, 'volume': 2876930}\n",
      "Forecast entry 2 for AXP: {'date': '2025-09-17', 'open': 301.62, 'high': 303.77, 'low': 298.74, 'close': 301.32, 'volume': 2515957}\n",
      "Forecast entry 3 for AXP: {'date': '2025-09-18', 'open': 301.22, 'high': 303.49, 'low': 297.38, 'close': 300.72, 'volume': 2874092}\n",
      "Forecast entry 4 for AXP: {'date': '2025-09-19', 'open': 301.07, 'high': 303.59, 'low': 297.2, 'close': 301.11, 'volume': 2735230}\n",
      "Prediction output for AXP: {\n",
      "  \"ticker\": \"AXP\",\n",
      "  \"last_date\": \"2025-09-12\",\n",
      "  \"future_window_days\": 5,\n",
      "  \"next_business_days\": [\n",
      "    \"2025-09-15\",\n",
      "    \"2025-09-16\",\n",
      "    \"2025-09-17\",\n",
      "    \"2025-09-18\",\n",
      "    \"2025-09-19\"\n",
      "  ],\n",
      "  \"predictions\": {\n",
      "    \"next_day\": {\n",
      "      \"open\": 300.74,\n",
      "      \"high\": 303.72,\n",
      "      \"low\": 298.17,\n",
      "      \"close\": 300.34,\n",
      "      \"volume\": 2454872\n",
      "    },\n",
      "    \"next_week\": {\n",
      "      \"high\": 303.77,\n",
      "      \"low\": 297.2\n",
      "    },\n",
      "    \"full_forecast\": [\n",
      "      {\n",
      "        \"date\": \"2025-09-15\",\n",
      "        \"open\": 300.74,\n",
      "        \"high\": 303.72,\n",
      "        \"low\": 298.17,\n",
      "        \"close\": 300.34,\n",
      "        \"volume\": 2454872\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-16\",\n",
      "        \"open\": 301.27,\n",
      "        \"high\": 303.59,\n",
      "        \"low\": 298.62,\n",
      "        \"close\": 302.24,\n",
      "        \"volume\": 2876930\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-17\",\n",
      "        \"open\": 301.62,\n",
      "        \"high\": 303.77,\n",
      "        \"low\": 298.74,\n",
      "        \"close\": 301.32,\n",
      "        \"volume\": 2515957\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-18\",\n",
      "        \"open\": 301.22,\n",
      "        \"high\": 303.49,\n",
      "        \"low\": 297.38,\n",
      "        \"close\": 300.72,\n",
      "        \"volume\": 2874092\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-19\",\n",
      "        \"open\": 301.07,\n",
      "        \"high\": 303.59,\n",
      "        \"low\": 297.2,\n",
      "        \"close\": 301.11,\n",
      "        \"volume\": 2735230\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "Forecast closes for AXP: [300.34, 302.24, 301.32, 300.72, 301.11]\n",
      "Last historical close for AXP: 325.30999755859375\n",
      "Y-values for plotting AXP: [325.30999755859375, 300.34, 302.24, 301.32, 300.72, 301.11]\n",
      "Plot saved for AXP at outputs/AXP/AXP_child_history_forecast.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lz/6w4cqrgn0cx_hw5sylb99fxh0000gn/T/ipykernel_1474/2361490311.py:215: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(**export_kwargs)\n",
      "/var/folders/lz/6w4cqrgn0cx_hw5sylb99fxh0000gn/T/ipykernel_1474/2361490311.py:397: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  last_close = float(last_close.item() if isinstance(last_close, np.ndarray) else last_close[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AXP → MSE: 0.07930, RMSE: 0.28161, R²: 0.92139\n",
      "✓ AXP model trained and saved to: outputs/AXP\n",
      "✓ Predictions saved to: outputs/AXP/AXP_child_forecast.json\n",
      "✓ Metrics saved to: outputs/AXP/AXP_child_metrics.json\n",
      "\n",
      "3. Generating fresh predictions...\n",
      "Fetched 5288 rows for GOOG\n",
      "Input shape for GOOG: (1, 60, 7)\n",
      "Prediction shape for GOOG: (1, 5, 7)\n",
      "Inverse transformed shape for GOOG: (5, 7)\n",
      "Forecast entry 0 for GOOG: {'date': '2025-09-15', 'open': 188.99, 'high': 193.69, 'low': 188.54, 'close': 190.51, 'volume': 104224232}\n",
      "Forecast entry 1 for GOOG: {'date': '2025-09-16', 'open': 189.0, 'high': 191.89, 'low': 187.64, 'close': 190.63, 'volume': 74471040}\n",
      "Forecast entry 2 for GOOG: {'date': '2025-09-17', 'open': 189.69, 'high': 192.71, 'low': 188.88, 'close': 189.86, 'volume': 70436792}\n",
      "Forecast entry 3 for GOOG: {'date': '2025-09-18', 'open': 190.41, 'high': 190.27, 'low': 187.7, 'close': 190.25, 'volume': 64933716}\n",
      "Forecast entry 4 for GOOG: {'date': '2025-09-19', 'open': 189.8, 'high': 192.29, 'low': 188.4, 'close': 189.59, 'volume': 59006616}\n",
      "Prediction output for GOOG: {\n",
      "  \"ticker\": \"GOOG\",\n",
      "  \"last_date\": \"2025-09-12\",\n",
      "  \"future_window_days\": 5,\n",
      "  \"next_business_days\": [\n",
      "    \"2025-09-15\",\n",
      "    \"2025-09-16\",\n",
      "    \"2025-09-17\",\n",
      "    \"2025-09-18\",\n",
      "    \"2025-09-19\"\n",
      "  ],\n",
      "  \"predictions\": {\n",
      "    \"next_day\": {\n",
      "      \"open\": 188.99,\n",
      "      \"high\": 193.69,\n",
      "      \"low\": 188.54,\n",
      "      \"close\": 190.51,\n",
      "      \"volume\": 104224232\n",
      "    },\n",
      "    \"next_week\": {\n",
      "      \"high\": 193.69,\n",
      "      \"low\": 187.64\n",
      "    },\n",
      "    \"full_forecast\": [\n",
      "      {\n",
      "        \"date\": \"2025-09-15\",\n",
      "        \"open\": 188.99,\n",
      "        \"high\": 193.69,\n",
      "        \"low\": 188.54,\n",
      "        \"close\": 190.51,\n",
      "        \"volume\": 104224232\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-16\",\n",
      "        \"open\": 189.0,\n",
      "        \"high\": 191.89,\n",
      "        \"low\": 187.64,\n",
      "        \"close\": 190.63,\n",
      "        \"volume\": 74471040\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-17\",\n",
      "        \"open\": 189.69,\n",
      "        \"high\": 192.71,\n",
      "        \"low\": 188.88,\n",
      "        \"close\": 189.86,\n",
      "        \"volume\": 70436792\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-18\",\n",
      "        \"open\": 190.41,\n",
      "        \"high\": 190.27,\n",
      "        \"low\": 187.7,\n",
      "        \"close\": 190.25,\n",
      "        \"volume\": 64933716\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-19\",\n",
      "        \"open\": 189.8,\n",
      "        \"high\": 192.29,\n",
      "        \"low\": 188.4,\n",
      "        \"close\": 189.59,\n",
      "        \"volume\": 59006616\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "✓ GOOG predictions for []:\n",
      "  Next-day open: $188.99\n",
      "  Next-day high: $193.69\n",
      "  Next-day low: $188.54\n",
      "  Next-day close: $190.51\n",
      "  Next-week high: $193.69\n",
      "  Next-week low: $187.64\n",
      "Fetched 5288 rows for AMZN\n",
      "Input shape for AMZN: (1, 60, 7)\n",
      "Prediction shape for AMZN: (1, 5, 7)\n",
      "Inverse transformed shape for AMZN: (5, 7)\n",
      "Forecast entry 0 for AMZN: {'date': '2025-09-15', 'open': 221.19, 'high': 224.61, 'low': 218.14, 'close': 221.15, 'volume': 40364460}\n",
      "Forecast entry 1 for AMZN: {'date': '2025-09-16', 'open': 221.34, 'high': 223.29, 'low': 218.16, 'close': 221.92, 'volume': 43643056}\n",
      "Forecast entry 2 for AMZN: {'date': '2025-09-17', 'open': 220.81, 'high': 225.06, 'low': 218.6, 'close': 220.9, 'volume': 46468348}\n",
      "Forecast entry 3 for AMZN: {'date': '2025-09-18', 'open': 222.1, 'high': 224.1, 'low': 217.59, 'close': 221.86, 'volume': 48109220}\n",
      "Forecast entry 4 for AMZN: {'date': '2025-09-19', 'open': 221.83, 'high': 222.63, 'low': 217.64, 'close': 220.58, 'volume': 46532400}\n",
      "Prediction output for AMZN: {\n",
      "  \"ticker\": \"AMZN\",\n",
      "  \"last_date\": \"2025-09-12\",\n",
      "  \"future_window_days\": 5,\n",
      "  \"next_business_days\": [\n",
      "    \"2025-09-15\",\n",
      "    \"2025-09-16\",\n",
      "    \"2025-09-17\",\n",
      "    \"2025-09-18\",\n",
      "    \"2025-09-19\"\n",
      "  ],\n",
      "  \"predictions\": {\n",
      "    \"next_day\": {\n",
      "      \"open\": 221.19,\n",
      "      \"high\": 224.61,\n",
      "      \"low\": 218.14,\n",
      "      \"close\": 221.15,\n",
      "      \"volume\": 40364460\n",
      "    },\n",
      "    \"next_week\": {\n",
      "      \"high\": 225.06,\n",
      "      \"low\": 217.59\n",
      "    },\n",
      "    \"full_forecast\": [\n",
      "      {\n",
      "        \"date\": \"2025-09-15\",\n",
      "        \"open\": 221.19,\n",
      "        \"high\": 224.61,\n",
      "        \"low\": 218.14,\n",
      "        \"close\": 221.15,\n",
      "        \"volume\": 40364460\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-16\",\n",
      "        \"open\": 221.34,\n",
      "        \"high\": 223.29,\n",
      "        \"low\": 218.16,\n",
      "        \"close\": 221.92,\n",
      "        \"volume\": 43643056\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-17\",\n",
      "        \"open\": 220.81,\n",
      "        \"high\": 225.06,\n",
      "        \"low\": 218.6,\n",
      "        \"close\": 220.9,\n",
      "        \"volume\": 46468348\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-18\",\n",
      "        \"open\": 222.1,\n",
      "        \"high\": 224.1,\n",
      "        \"low\": 217.59,\n",
      "        \"close\": 221.86,\n",
      "        \"volume\": 48109220\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-19\",\n",
      "        \"open\": 221.83,\n",
      "        \"high\": 222.63,\n",
      "        \"low\": 217.64,\n",
      "        \"close\": 220.58,\n",
      "        \"volume\": 46532400\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "✓ AMZN predictions for []:\n",
      "  Next-day open: $221.19\n",
      "  Next-day high: $224.61\n",
      "  Next-day low: $218.14\n",
      "  Next-day close: $221.15\n",
      "  Next-week high: $225.06\n",
      "  Next-week low: $217.59\n",
      "Fetched 3336 rows for META\n",
      "Input shape for META: (1, 60, 7)\n",
      "Prediction shape for META: (1, 5, 7)\n",
      "Inverse transformed shape for META: (5, 7)\n",
      "Forecast entry 0 for META: {'date': '2025-09-15', 'open': 685.84, 'high': 694.6, 'low': 677.42, 'close': 689.82, 'volume': 17689582}\n",
      "Forecast entry 1 for META: {'date': '2025-09-16', 'open': 691.95, 'high': 696.01, 'low': 680.76, 'close': 692.31, 'volume': 17389508}\n",
      "Forecast entry 2 for META: {'date': '2025-09-17', 'open': 687.53, 'high': 699.73, 'low': 678.2, 'close': 689.07, 'volume': 18255380}\n",
      "Forecast entry 3 for META: {'date': '2025-09-18', 'open': 692.86, 'high': 705.45, 'low': 684.07, 'close': 693.82, 'volume': 18800114}\n",
      "Forecast entry 4 for META: {'date': '2025-09-19', 'open': 698.73, 'high': 703.92, 'low': 683.11, 'close': 696.67, 'volume': 18400848}\n",
      "Prediction output for META: {\n",
      "  \"ticker\": \"META\",\n",
      "  \"last_date\": \"2025-09-12\",\n",
      "  \"future_window_days\": 5,\n",
      "  \"next_business_days\": [\n",
      "    \"2025-09-15\",\n",
      "    \"2025-09-16\",\n",
      "    \"2025-09-17\",\n",
      "    \"2025-09-18\",\n",
      "    \"2025-09-19\"\n",
      "  ],\n",
      "  \"predictions\": {\n",
      "    \"next_day\": {\n",
      "      \"open\": 685.84,\n",
      "      \"high\": 694.6,\n",
      "      \"low\": 677.42,\n",
      "      \"close\": 689.82,\n",
      "      \"volume\": 17689582\n",
      "    },\n",
      "    \"next_week\": {\n",
      "      \"high\": 705.45,\n",
      "      \"low\": 677.42\n",
      "    },\n",
      "    \"full_forecast\": [\n",
      "      {\n",
      "        \"date\": \"2025-09-15\",\n",
      "        \"open\": 685.84,\n",
      "        \"high\": 694.6,\n",
      "        \"low\": 677.42,\n",
      "        \"close\": 689.82,\n",
      "        \"volume\": 17689582\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-16\",\n",
      "        \"open\": 691.95,\n",
      "        \"high\": 696.01,\n",
      "        \"low\": 680.76,\n",
      "        \"close\": 692.31,\n",
      "        \"volume\": 17389508\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-17\",\n",
      "        \"open\": 687.53,\n",
      "        \"high\": 699.73,\n",
      "        \"low\": 678.2,\n",
      "        \"close\": 689.07,\n",
      "        \"volume\": 18255380\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-18\",\n",
      "        \"open\": 692.86,\n",
      "        \"high\": 705.45,\n",
      "        \"low\": 684.07,\n",
      "        \"close\": 693.82,\n",
      "        \"volume\": 18800114\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-19\",\n",
      "        \"open\": 698.73,\n",
      "        \"high\": 703.92,\n",
      "        \"low\": 683.11,\n",
      "        \"close\": 696.67,\n",
      "        \"volume\": 18400848\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "✓ META predictions for []:\n",
      "  Next-day open: $685.84\n",
      "  Next-day high: $694.60\n",
      "  Next-day low: $677.42\n",
      "  Next-day close: $689.82\n",
      "  Next-week high: $705.45\n",
      "  Next-week low: $677.42\n",
      "Fetched 5288 rows for AXP\n",
      "Input shape for AXP: (1, 60, 7)\n",
      "Prediction shape for AXP: (1, 5, 7)\n",
      "Inverse transformed shape for AXP: (5, 7)\n",
      "Forecast entry 0 for AXP: {'date': '2025-09-15', 'open': 300.74, 'high': 303.72, 'low': 298.17, 'close': 300.34, 'volume': 2454872}\n",
      "Forecast entry 1 for AXP: {'date': '2025-09-16', 'open': 301.27, 'high': 303.59, 'low': 298.62, 'close': 302.24, 'volume': 2876930}\n",
      "Forecast entry 2 for AXP: {'date': '2025-09-17', 'open': 301.62, 'high': 303.77, 'low': 298.74, 'close': 301.32, 'volume': 2515957}\n",
      "Forecast entry 3 for AXP: {'date': '2025-09-18', 'open': 301.22, 'high': 303.49, 'low': 297.38, 'close': 300.72, 'volume': 2874092}\n",
      "Forecast entry 4 for AXP: {'date': '2025-09-19', 'open': 301.07, 'high': 303.59, 'low': 297.2, 'close': 301.11, 'volume': 2735230}\n",
      "Prediction output for AXP: {\n",
      "  \"ticker\": \"AXP\",\n",
      "  \"last_date\": \"2025-09-12\",\n",
      "  \"future_window_days\": 5,\n",
      "  \"next_business_days\": [\n",
      "    \"2025-09-15\",\n",
      "    \"2025-09-16\",\n",
      "    \"2025-09-17\",\n",
      "    \"2025-09-18\",\n",
      "    \"2025-09-19\"\n",
      "  ],\n",
      "  \"predictions\": {\n",
      "    \"next_day\": {\n",
      "      \"open\": 300.74,\n",
      "      \"high\": 303.72,\n",
      "      \"low\": 298.17,\n",
      "      \"close\": 300.34,\n",
      "      \"volume\": 2454872\n",
      "    },\n",
      "    \"next_week\": {\n",
      "      \"high\": 303.77,\n",
      "      \"low\": 297.2\n",
      "    },\n",
      "    \"full_forecast\": [\n",
      "      {\n",
      "        \"date\": \"2025-09-15\",\n",
      "        \"open\": 300.74,\n",
      "        \"high\": 303.72,\n",
      "        \"low\": 298.17,\n",
      "        \"close\": 300.34,\n",
      "        \"volume\": 2454872\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-16\",\n",
      "        \"open\": 301.27,\n",
      "        \"high\": 303.59,\n",
      "        \"low\": 298.62,\n",
      "        \"close\": 302.24,\n",
      "        \"volume\": 2876930\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-17\",\n",
      "        \"open\": 301.62,\n",
      "        \"high\": 303.77,\n",
      "        \"low\": 298.74,\n",
      "        \"close\": 301.32,\n",
      "        \"volume\": 2515957\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-18\",\n",
      "        \"open\": 301.22,\n",
      "        \"high\": 303.49,\n",
      "        \"low\": 297.38,\n",
      "        \"close\": 300.72,\n",
      "        \"volume\": 2874092\n",
      "      },\n",
      "      {\n",
      "        \"date\": \"2025-09-19\",\n",
      "        \"open\": 301.07,\n",
      "        \"high\": 303.59,\n",
      "        \"low\": 297.2,\n",
      "        \"close\": 301.11,\n",
      "        \"volume\": 2735230\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "✓ AXP predictions for []:\n",
      "  Next-day open: $300.74\n",
      "  Next-day high: $303.72\n",
      "  Next-day low: $298.17\n",
      "  Next-day close: $300.34\n",
      "  Next-week high: $303.77\n",
      "  Next-week low: $297.20\n",
      "\n",
      "==================================================\n",
      "Pipeline completed! Check 'outputs/' directory for models, scalers, predictions, metrics, and plots.\n",
      "\n",
      "File structure:\n",
      "outputs/\n",
      "├── parent/\n",
      "│   ├── model.pt\n",
      "│   ├── model.onnx\n",
      "│   ├── parent_scaler.pkl\n",
      "│   └── GSPC_parent_metrics.json\n",
      "├── GOOG/\n",
      "│   ├── model.pt\n",
      "│   ├── model.onnx\n",
      "│   ├── GOOG_child_scaler.pkl\n",
      "│   ├── GOOG_child_forecast.json\n",
      "│   ├── GOOG_child_metrics.json\n",
      "│   └── GOOG_child_history_forecast.png\n",
      "├── AMZN/\n",
      "│   ├── model.pt\n",
      "│   ├── model.onnx\n",
      "│   ├── AMZN_child_scaler.pkl\n",
      "│   ├── AMZN_child_forecast.json\n",
      "│   ├── AMZN_child_metrics.json\n",
      "│   └── AMZN_child_history_forecast.png\n",
      "├── META/\n",
      "│   ├── model.pt\n",
      "│   ├── model.onnx\n",
      "│   ├── META_child_scaler.pkl\n",
      "│   ├── META_child_forecast.json\n",
      "│   ├── META_child_metrics.json\n",
      "│   └── META_child_history_forecast.png\n",
      "├── AXP/\n",
      "│   ├── model.pt\n",
      "│   ├── model.onnx\n",
      "│   ├── AXP_child_scaler.pkl\n",
      "│   ├── AXP_child_forecast.json\n",
      "│   ├── AXP_child_metrics.json\n",
      "│   └── AXP_child_history_forecast.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import ta\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import warnings\n",
    "\n",
    "# Suppress ONNX export warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.onnx\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"torch.onnx\")\n",
    "\n",
    "# Check PyTorch version for dynamo compatibility\n",
    "PYTORCH_VERSION = torch.__version__.split('+')[0]\n",
    "PYTORCH_MAJOR, PYTORCH_MINOR = map(int, PYTORCH_VERSION.split('.')[:2])\n",
    "USE_DYNAMO = PYTORCH_MAJOR >= 2 and PYTORCH_MINOR >= 9\n",
    "\n",
    "# Configuration\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for the stock prediction pipeline.\"\"\"\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    context_len: int = 60\n",
    "    pred_len: int = 5\n",
    "    features: List[str] = field(default_factory=lambda: [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"RSI14\", \"MACD\"])\n",
    "    batch_size: int = 32\n",
    "    parent_ticker: str = \"^GSPC\"\n",
    "    child_tickers: List[str] = field(default_factory=lambda: [\"GOOG\", \"AMZN\", \"META\", \"AXP\"])\n",
    "    start_date: str = \"2004-08-19\"  # Google's IPO date\n",
    "    parent_epochs: int = 20\n",
    "    child_epochs: int = 10\n",
    "    parent_dir: str = \"outputs/parent\"\n",
    "    workdir: str = \"outputs\"\n",
    "\n",
    "    @property\n",
    "    def input_size(self) -> int:\n",
    "        return len(self.features)\n",
    "\n",
    "# Custom exception\n",
    "class PipelineError(Exception):\n",
    "    \"\"\"Custom exception for pipeline errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Initialize directories\n",
    "def initialize_dirs():\n",
    "    \"\"\"Initialize output directories.\"\"\"\n",
    "    config = Config()\n",
    "    try:\n",
    "        os.makedirs(config.workdir, exist_ok=True)\n",
    "        os.makedirs(config.parent_dir, exist_ok=True)\n",
    "        for ticker in config.child_tickers:\n",
    "            os.makedirs(os.path.join(config.workdir, ticker), exist_ok=True)\n",
    "    except Exception as e:\n",
    "        raise PipelineError(f\"Failed to create output directories: {e}\")\n",
    "\n",
    "initialize_dirs()\n",
    "load_dotenv()\n",
    "\n",
    "def rsi(series: pd.Series, period: int = 14) -> pd.Series:\n",
    "    \"\"\"Calculate RSI for a given series.\"\"\"\n",
    "    delta = series.diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(window=period).mean()\n",
    "    loss = -delta.where(delta < 0, 0).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - 100 / (1 + rs)\n",
    "\n",
    "def macd(series: pd.Series, fast: int = 12, slow: int = 26) -> pd.Series:\n",
    "    \"\"\"Calculate MACD for a given series.\"\"\"\n",
    "    ema_fast = series.ewm(span=fast, adjust=False).mean()\n",
    "    ema_slow = series.ewm(span=slow, adjust=False).mean()\n",
    "    return ema_fast - ema_slow\n",
    "\n",
    "def fetch_ohlcv(ticker: str, start: str = Config().start_date, end: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Fetch OHLCV data with technical indicators.\"\"\"\n",
    "    config = Config()\n",
    "    try:\n",
    "        df = yf.download(ticker, start=start, end=end, interval=\"1d\", auto_adjust=True, progress=False)\n",
    "        if df.empty:\n",
    "            raise PipelineError(f\"No data downloaded for {ticker}\")\n",
    "        df = df.reset_index().rename(columns={\"Date\": \"date\"})\n",
    "        df = df[[\"date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].dropna()\n",
    "        df[\"RSI14\"] = rsi(df[\"Close\"])\n",
    "        df[\"MACD\"] = macd(df[\"Close\"])\n",
    "        df = df[[\"date\"] + config.features].dropna()\n",
    "        \n",
    "        # Validate data\n",
    "        if len(df) < config.context_len + config.pred_len:\n",
    "            raise PipelineError(f\"Insufficient data for {ticker}: {len(df)} rows, need at least {config.context_len + config.pred_len}\")\n",
    "        if df[config.features].isnull().any().any():\n",
    "            raise PipelineError(f\"NaN values found in features for {ticker}\")\n",
    "        if not df[config.features].apply(lambda x: pd.api.types.is_numeric_dtype(x)).all():\n",
    "            raise PipelineError(f\"Non-numeric values found in features for {ticker}\")\n",
    "        print(f\"Fetched {len(df)} rows for {ticker}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise PipelineError(f\"Failed to fetch data for {ticker}: {e}\")\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    \"\"\"Dataset for stock price sequences.\"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, scaler: StandardScaler, context_len: int = Config().context_len, pred_len: int = Config().pred_len):\n",
    "        self.context_len = context_len\n",
    "        self.pred_len = pred_len\n",
    "        try:\n",
    "            vals = scaler.transform(df[Config().features]).astype(\"float32\")\n",
    "            self.samples = []\n",
    "            for t in range(context_len, len(df) - pred_len):\n",
    "                past = vals[t - context_len:t]\n",
    "                fut = vals[t:t + pred_len]\n",
    "                if past.shape == (context_len, len(Config().features)) and fut.shape == (pred_len, len(Config().features)):\n",
    "                    self.samples.append((past, fut))\n",
    "                else:\n",
    "                    print(f\"Skipping invalid sample at index {t}: past shape {past.shape}, fut shape {fut.shape}\")\n",
    "            if not self.samples:\n",
    "                raise PipelineError(\"No valid samples created for dataset\")\n",
    "        except Exception as e:\n",
    "            raise PipelineError(f\"Failed to create dataset: {e}\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        past, fut = self.samples[idx]\n",
    "        return torch.tensor(past), torch.tensor(fut)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"LSTM model for stock price prediction.\"\"\"\n",
    "    def __init__(self, input_size: int = Config().input_size, hidden_size: int = 128, num_layers: int = 3, \n",
    "                 pred_len: int = Config().pred_len, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, input_size * pred_len)\n",
    "        self.pred_len = pred_len\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out.view(-1, self.pred_len, self.input_size)\n",
    "\n",
    "def fit_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, epochs: int = 8, lr: float = 1e-3) -> nn.Module:\n",
    "    \"\"\"Train the LSTM model with early stopping.\"\"\"\n",
    "    model.to(Config().device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=3)\n",
    "    best_val_loss = float('inf')\n",
    "    patience, counter = 5, 0\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for X, Y in train_loader:\n",
    "            X, Y = X.to(Config().device), Y.to(Config().device)\n",
    "            opt.zero_grad()\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, Y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {ep}/{epochs} - Train Loss: {avg_train_loss:.5f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X, Y in val_loader:\n",
    "                X, Y = X.to(Config().device), Y.to(Config().device)\n",
    "                pred = model(X)\n",
    "                val_loss += criterion(pred, Y).item()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {ep}/{epochs} - Val Loss: {avg_val_loss:.5f}\")\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    return model\n",
    "\n",
    "def model_to_onnx(model: nn.Module, path: str):\n",
    "    \"\"\"Convert PyTorch model to ONNX and create inference session.\"\"\"\n",
    "    try:\n",
    "        model.eval()\n",
    "        dummy_input = torch.randn(1, Config().context_len, Config().input_size).to(Config().device)\n",
    "        export_kwargs = {\n",
    "            \"model\": model,\n",
    "            \"args\": dummy_input,\n",
    "            \"f\": path,\n",
    "            \"input_names\": ['input'],\n",
    "            \"output_names\": ['output'],\n",
    "            \"dynamic_axes\": {'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},\n",
    "            \"opset_version\": 12\n",
    "        }\n",
    "        if USE_DYNAMO:\n",
    "            export_kwargs[\"dynamo\"] = True\n",
    "        torch.onnx.export(**export_kwargs)\n",
    "        return ort.InferenceSession(path)\n",
    "    except Exception as e:\n",
    "        raise PipelineError(f\"Failed to export model to ONNX at {path}: {e}\")\n",
    "\n",
    "def save_model(model: nn.Module, scaler: StandardScaler, path: str, model_type: str = \"parent\", ticker: Optional[str] = None):\n",
    "    \"\"\"Save model, scaler, and ONNX model.\"\"\"\n",
    "    try:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        torch_save_path = os.path.join(path, \"model.pt\")\n",
    "        scaler_filename = \"parent_scaler.pkl\" if model_type == \"parent\" else f\"{ticker}_child_scaler.pkl\"\n",
    "        scaler_path = os.path.join(path, scaler_filename)\n",
    "        onnx_path = os.path.join(path, \"model.onnx\")\n",
    "\n",
    "        torch.save(model.state_dict(), torch_save_path)\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "        model_to_onnx(model, onnx_path)\n",
    "        print(f\"Model and scaler saved locally at {path}\")\n",
    "    except Exception as e:\n",
    "        raise PipelineError(f\"Failed to save model at {path}: {e}\")\n",
    "\n",
    "def load_model(path: str, model_type: str = \"parent\", ticker: Optional[str] = None) -> Tuple[ort.InferenceSession, StandardScaler]:\n",
    "    \"\"\"Load ONNX model and scaler from path.\"\"\"\n",
    "    if model_type == \"child\" and not ticker:\n",
    "        raise ValueError(\"Ticker must be provided for child model\")\n",
    "    \n",
    "    scaler_filename = \"parent_scaler.pkl\" if model_type == \"parent\" else f\"{ticker}_child_scaler.pkl\"\n",
    "    onnx_path = os.path.join(path, \"model.onnx\")\n",
    "    scaler_path = os.path.join(path, scaler_filename)\n",
    "    \n",
    "    if os.path.exists(onnx_path) and os.path.exists(scaler_path):\n",
    "        return ort.InferenceSession(onnx_path), joblib.load(scaler_path)\n",
    "    raise FileNotFoundError(f\"ONNX model or scaler not found at {path}\")\n",
    "\n",
    "def predict_one_step_and_week(session: ort.InferenceSession, df: pd.DataFrame, scaler: StandardScaler, ticker: str) -> Dict:\n",
    "    \"\"\"Predict next day and week OHLCV values.\"\"\"\n",
    "    try:\n",
    "        config = Config()\n",
    "        vals = scaler.transform(df[config.features]).astype(\"float32\")\n",
    "        if vals.shape[0] < config.context_len:\n",
    "            raise PipelineError(f\"Insufficient data: {vals.shape[0]} rows, need at least {config.context_len}\")\n",
    "        X = vals[-config.context_len:].reshape(1, config.context_len, config.input_size)\n",
    "        print(f\"Input shape for {ticker}: {X.shape}\")  # Debug\n",
    "\n",
    "        pred = session.run(None, {'input': X})[0]\n",
    "        print(f\"Prediction shape for {ticker}: {pred.shape}\")  # Debug\n",
    "        if pred.shape != (1, config.pred_len, config.input_size):\n",
    "            raise PipelineError(f\"Unexpected prediction shape for {ticker}: {pred.shape}\")\n",
    "\n",
    "        pred_full = scaler.inverse_transform(pred.reshape(-1, config.input_size))\n",
    "        print(f\"Inverse transformed shape for {ticker}: {pred_full.shape}\")  # Debug\n",
    "        pred_full = pred_full.reshape(config.pred_len, config.input_size)\n",
    "        pred = pred_full[:, :5]  # OHLCV only\n",
    "\n",
    "        # Validate predictions\n",
    "        if np.any(np.isnan(pred)):\n",
    "            raise PipelineError(f\"NaN values in predictions for {ticker}\")\n",
    "\n",
    "        last_date = pd.to_datetime(df[\"date\"].iloc[-1])\n",
    "        next_business_days = pd.bdate_range(last_date + pd.Timedelta(days=1), periods=config.pred_len)\n",
    "        next_business_days_str = [str(d.date()) for d in next_business_days]\n",
    "\n",
    "        full_forecast = []\n",
    "        for i, d in enumerate(next_business_days):\n",
    "            forecast_entry = {\n",
    "                \"date\": str(d.date()),\n",
    "                \"open\": round(float(pred[i, 0]), 2),\n",
    "                \"high\": round(float(pred[i, 1]), 2),\n",
    "                \"low\": round(float(pred[i, 2]), 2),\n",
    "                \"close\": round(float(pred[i, 3]), 2),\n",
    "                \"volume\": int(pred[i, 4])\n",
    "            }\n",
    "            print(f\"Forecast entry {i} for {ticker}: {forecast_entry}\")  # Debug\n",
    "            full_forecast.append(forecast_entry)\n",
    "\n",
    "        output = {\n",
    "            \"ticker\": ticker,\n",
    "            \"last_date\": str(last_date.date()),\n",
    "            \"future_window_days\": config.pred_len,\n",
    "            \"next_business_days\": next_business_days_str,\n",
    "            \"predictions\": {\n",
    "                \"next_day\": {\n",
    "                    \"open\": round(float(pred[0, 0]), 2),\n",
    "                    \"high\": round(float(pred[0, 1]), 2),\n",
    "                    \"low\": round(float(pred[0, 2]), 2),\n",
    "                    \"close\": round(float(pred[0, 3]), 2),\n",
    "                    \"volume\": int(pred[0, 4])\n",
    "                },\n",
    "                \"next_week\": {\n",
    "                    \"high\": round(float(np.max(pred[:, 1])), 2),\n",
    "                    \"low\": round(float(np.min(pred[:, 2])), 2)\n",
    "                },\n",
    "                \"full_forecast\": full_forecast\n",
    "            }\n",
    "        }\n",
    "        print(f\"Prediction output for {ticker}: {json.dumps(output, indent=2)}\")  # Debug\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        print(f\"Prediction failed for {ticker}: {e}\")\n",
    "        return {\"ticker\": ticker, \"error\": str(e)}\n",
    "\n",
    "def evaluate_model(session: ort.InferenceSession, df: pd.DataFrame, scaler: StandardScaler, out_dir: str, ticker: str) -> Dict:\n",
    "    \"\"\"Evaluate model performance.\"\"\"\n",
    "    try:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        config = Config()\n",
    "        vals = scaler.transform(df[config.features]).astype(\"float32\")\n",
    "        X, Y = [], []\n",
    "        for t in range(config.context_len, len(vals) - config.pred_len):\n",
    "            past = vals[t - config.context_len:t]\n",
    "            fut = vals[t:t + config.pred_len]\n",
    "            if past.shape == (config.context_len, config.input_size) and fut.shape == (config.pred_len, config.input_size):\n",
    "                X.append(past)\n",
    "                Y.append(fut)\n",
    "            else:\n",
    "                print(f\"Skipping invalid evaluation sample at index {t}: past shape {past.shape}, fut shape {fut.shape}\")\n",
    "\n",
    "        if not X:\n",
    "            print(f\"No valid samples for evaluation for {ticker}\")\n",
    "            return {}\n",
    "\n",
    "        X, Y = np.array(X), np.array(Y)\n",
    "        preds = [session.run(None, {'input': x.reshape(1, config.context_len, config.input_size)})[0] for x in X]\n",
    "        preds = np.array(preds)\n",
    "        Y_ohlcv = Y.reshape(-1, config.input_size)[:, :5]\n",
    "        preds_ohlcv = preds.reshape(-1, config.input_size)[:, :5]\n",
    "\n",
    "        mse = mean_squared_error(Y_ohlcv, preds_ohlcv)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(Y_ohlcv, preds_ohlcv)\n",
    "\n",
    "        metrics = {\"MSE\": mse, \"RMSE\": rmse, \"R2\": r2}\n",
    "        metrics_filename = f\"{ticker}_parent_metrics.json\" if \"parent\" in out_dir else f\"{ticker}_child_metrics.json\"\n",
    "        metrics_path = os.path.join(out_dir, metrics_filename)\n",
    "        with open(metrics_path, \"w\") as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        print(f\"{ticker} → MSE: {mse:.5f}, RMSE: {rmse:.5f}, R²: {r2:.5f}\")\n",
    "\n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation failed for {ticker}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def save_json(payload: Dict, path: str) -> str:\n",
    "    \"\"\"Save dictionary to JSON file.\"\"\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(payload, f, indent=2)\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        raise PipelineError(f\"Failed to save JSON at {path}: {e}\")\n",
    "\n",
    "def plot_outputs(df: pd.DataFrame, payload: Dict, out_dir: str, ticker: str):\n",
    "    \"\"\"Plot historical and forecasted prices.\"\"\"\n",
    "    try:\n",
    "        if \"error\" in payload:\n",
    "            raise PipelineError(f\"Cannot plot for {ticker}: prediction failed with error {payload['error']}\")\n",
    "\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(df[\"date\"], df[\"Close\"], label=\"History\")\n",
    "        \n",
    "        ndo = payload[\"predictions\"][\"next_day\"][\"open\"]\n",
    "        ndc = payload[\"predictions\"][\"next_day\"][\"close\"]\n",
    "        ndh = payload[\"predictions\"][\"next_day\"][\"high\"]\n",
    "        ndl = payload[\"predictions\"][\"next_day\"][\"low\"]\n",
    "        whi = payload[\"predictions\"][\"next_week\"][\"high\"]\n",
    "        wlo = payload[\"predictions\"][\"next_week\"][\"low\"]\n",
    "        \n",
    "        # Validate forecast_closes\n",
    "        forecast_closes = []\n",
    "        for p in payload[\"predictions\"][\"full_forecast\"]:\n",
    "            close = p.get(\"close\")\n",
    "            if not isinstance(close, (int, float)) or pd.isna(close):\n",
    "                raise PipelineError(f\"Invalid close value in full_forecast for {ticker}: {close}\")\n",
    "            forecast_closes.append(float(close))\n",
    "        print(f\"Forecast closes for {ticker}: {forecast_closes}\")\n",
    "        \n",
    "        # Ensure last historical close is a scalar\n",
    "        last_close = df[\"Close\"].iloc[-1]\n",
    "        if isinstance(last_close, (np.ndarray, list, pd.Series)):\n",
    "            last_close = float(last_close.item() if isinstance(last_close, np.ndarray) else last_close[0])\n",
    "        elif not isinstance(last_close, (int, float)):\n",
    "            raise PipelineError(f\"Invalid last historical close for {ticker}: {last_close}\")\n",
    "        print(f\"Last historical close for {ticker}: {last_close}\")\n",
    "        \n",
    "        # Validate date arrays\n",
    "        last_date = pd.to_datetime(payload[\"last_date\"])\n",
    "        next_dates = [pd.to_datetime(d) for d in payload[\"next_business_days\"]]\n",
    "        y_values = [last_close] + forecast_closes\n",
    "        print(f\"Y-values for plotting {ticker}: {y_values}\")\n",
    "        \n",
    "        plt.axhline(ndo, color=\"orange\", linestyle=\"-\", alpha=0.7, label=\"Next-day open\")\n",
    "        plt.axhline(ndc, color=\"r\", linestyle=\"--\", label=\"Next-day close\")\n",
    "        plt.axhline(ndh, color=\"darkgreen\", linestyle=\"-\", alpha=0.7, label=\"Next-day high\")\n",
    "        plt.axhline(ndl, color=\"darkred\", linestyle=\"-\", alpha=0.7, label=\"Next-day low\")\n",
    "        plt.axhline(whi, color=\"g\", linestyle=\":\", label=\"Next-week high\")\n",
    "        plt.axhline(wlo, color=\"b\", linestyle=\":\", label=\"Next-week low\")\n",
    "        \n",
    "        plt.plot([last_date] + next_dates, y_values, 'm--', label=\"Multi-step forecast closes\")\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.title(f\"{ticker} Close + Next Day & Week Forecast\")\n",
    "        plot_filename = f\"{ticker}_parent_history_forecast.png\" if \"parent\" in out_dir else f\"{ticker}_child_history_forecast.png\"\n",
    "        plot_path = os.path.join(out_dir, plot_filename)\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\"Plot saved for {ticker} at {plot_path}\")\n",
    "    except Exception as e:\n",
    "        raise PipelineError(f\"Plotting failed for {ticker}: {e}\")\n",
    "\n",
    "def train_parent(ticker: str = Config().parent_ticker, start: str = Config().start_date, \n",
    "                 epochs: int = Config().parent_epochs, out_dir: str = Config().parent_dir) -> Dict:\n",
    "    \"\"\"Train parent model.\"\"\"\n",
    "    try:\n",
    "        df = fetch_ohlcv(ticker, start)\n",
    "        scaler = StandardScaler().fit(df[Config().features])\n",
    "        dataset = StockDataset(df, scaler)\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "        train_loader = DataLoader(train_dataset, batch_size=Config().batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=Config().batch_size)\n",
    "\n",
    "        model = LSTMModel()\n",
    "        model = fit_model(model, train_loader, val_loader, epochs=epochs, lr=1e-3)\n",
    "        save_model(model, scaler, out_dir, model_type=\"parent\", ticker=ticker)\n",
    "        session = model_to_onnx(model, os.path.join(out_dir, \"model.onnx\"))\n",
    "        evaluate_model(session, df, scaler, out_dir, ticker.replace(\"^\", \"\"))\n",
    "        return {\"checkpoint\": out_dir}\n",
    "    except Exception as e:\n",
    "        raise PipelineError(f\"Parent model training failed for {ticker}: {e}\")\n",
    "\n",
    "def train_child(ticker: str, start: str = Config().start_date, epochs: int = Config().child_epochs, \n",
    "                parent_dir: str = Config().parent_dir, workdir: str = Config().workdir) -> Dict:\n",
    "    \"\"\"Train child model using parent model weights.\"\"\"\n",
    "    try:\n",
    "        df = fetch_ohlcv(ticker, start)\n",
    "        parent_model = LSTMModel()\n",
    "        parent_model_path = os.path.join(parent_dir, \"model.pt\")\n",
    "        if not os.path.exists(parent_model_path):\n",
    "            raise FileNotFoundError(f\"Parent model not found at {parent_model_path}\")\n",
    "        parent_model.load_state_dict(torch.load(parent_model_path, map_location=Config().device))\n",
    "\n",
    "        for name, param in parent_model.named_parameters():\n",
    "            if \"lstm\" in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        scaler = StandardScaler().fit(df[Config().features])\n",
    "        if (df[Config().features].std() == 0).any():\n",
    "            raise PipelineError(f\"Zero variance in features for {ticker}\")\n",
    "        dataset = StockDataset(df, scaler)\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "        train_loader = DataLoader(train_dataset, batch_size=Config().batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=Config().batch_size)\n",
    "\n",
    "        child_model = fit_model(parent_model, train_loader, val_loader, epochs=epochs, lr=3e-4)\n",
    "        child_dir = os.path.join(workdir, ticker)\n",
    "        save_model(child_model, scaler, child_dir, model_type=\"child\", ticker=ticker)\n",
    "\n",
    "        session = model_to_onnx(child_model, os.path.join(child_dir, \"model.onnx\"))\n",
    "        payload = predict_one_step_and_week(session, df, scaler, ticker)\n",
    "        json_filename = f\"{ticker}_child_forecast.json\"\n",
    "        json_path = save_json(payload, os.path.join(child_dir, json_filename))\n",
    "        plot_outputs(df, payload, child_dir, ticker)\n",
    "        evaluate_model(session, df, scaler, child_dir, ticker)\n",
    "        return {\"checkpoint\": child_dir, \"json\": json_path}\n",
    "    except Exception as e:\n",
    "        raise PipelineError(f\"Child model training failed for {ticker}: {e}\")\n",
    "\n",
    "def predict_child(ticker: str, parent_dir: str = Config().parent_dir, workdir: str = Config().workdir) -> Dict:\n",
    "    \"\"\"Predict using child model.\"\"\"\n",
    "    try:\n",
    "        child_dir = os.path.join(workdir, ticker)\n",
    "        df = fetch_ohlcv(ticker)\n",
    "        session, scaler = load_model(path=child_dir, model_type=\"child\", ticker=ticker)\n",
    "        return predict_one_step_and_week(session, df, scaler, ticker)\n",
    "    except Exception as e:\n",
    "        print(f\"Prediction failed for {ticker}: {e}\")\n",
    "        return {\"ticker\": ticker, \"error\": str(e)}\n",
    "\n",
    "def infer_child_stock(\n",
    "    ticker: str,\n",
    "    start: str = Config().start_date,\n",
    "    epochs: int = Config().child_epochs,\n",
    "    parent_dir: str = Config().parent_dir,\n",
    "    workdir: str = Config().workdir,\n",
    "    train_if_not_exists: bool = True,\n",
    "    return_plot: bool = False,\n",
    "    return_metrics: bool = False\n",
    ") -> Dict:\n",
    "    \"\"\"Infer stock predictions, training if necessary.\"\"\"\n",
    "    child_dir = os.path.join(workdir, ticker)\n",
    "    onnx_path = os.path.join(child_dir, \"model.onnx\")\n",
    "    scaler_path = os.path.join(child_dir, f\"{ticker}_child_scaler.pkl\")\n",
    "    \n",
    "    model_exists = os.path.exists(onnx_path) and os.path.exists(scaler_path)\n",
    "    \n",
    "    try:\n",
    "        session, scaler = load_model(path=child_dir, model_type=\"child\", ticker=ticker)\n",
    "        print(f\"Loaded model for {ticker} from {child_dir}\")\n",
    "    except FileNotFoundError as e:\n",
    "        if train_if_not_exists:\n",
    "            print(f\"Model for {ticker} not found. Training now...\")\n",
    "            try:\n",
    "                train_summary = train_child(ticker, start, epochs, parent_dir, workdir)\n",
    "                child_dir = train_summary[\"checkpoint\"]\n",
    "                session, scaler = load_model(path=child_dir, model_type=\"child\", ticker=ticker)\n",
    "            except Exception as e:\n",
    "                raise PipelineError(f\"Failed to train child model for {ticker}: {e}\")\n",
    "        else:\n",
    "            raise PipelineError(f\"Child model for {ticker} not found in {child_dir}.\") from e\n",
    "    \n",
    "    print(f\"Running inference for {ticker}...\")\n",
    "    df = fetch_ohlcv(ticker, start)\n",
    "    predictions = predict_one_step_and_week(session, df, scaler, ticker)\n",
    "    \n",
    "    output = {\"ticker\": ticker, \"predictions\": predictions}\n",
    "    \n",
    "    if return_metrics:\n",
    "        output[\"metrics\"] = evaluate_model(session, df, scaler, child_dir, ticker)\n",
    "    \n",
    "    if return_plot:\n",
    "        plot_outputs(df, predictions, child_dir, ticker)\n",
    "        output[\"plot_path\"] = os.path.join(child_dir, f\"{ticker}_child_history_forecast.png\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the stock prediction pipeline.\"\"\"\n",
    "    config = Config()\n",
    "    \n",
    "    # Train parent model\n",
    "    print(\"1. Training parent model for S&P 500...\")\n",
    "    parent_model_path = os.path.join(config.parent_dir, \"model.pt\")\n",
    "    parent_scaler_path = os.path.join(config.parent_dir, \"parent_scaler.pkl\")\n",
    "    parent_onnx_path = os.path.join(config.parent_dir, \"model.onnx\")\n",
    "    if os.path.exists(parent_model_path) and os.path.exists(parent_scaler_path) and os.path.exists(parent_onnx_path):\n",
    "        print(f\"✓ Using existing parent model at: {config.parent_dir}\")\n",
    "    else:\n",
    "        try:\n",
    "            parent_summary = train_parent(ticker=config.parent_ticker, start=config.start_date, \n",
    "                                        epochs=config.parent_epochs, out_dir=config.parent_dir)\n",
    "            print(f\"✓ Parent model trained and saved to: {config.parent_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error training parent model: {e}\")\n",
    "            if os.path.exists(parent_model_path) and os.path.exists(parent_scaler_path):\n",
    "                print(f\"✓ Found existing parent model at: {config.parent_dir}. Continuing...\")\n",
    "            else:\n",
    "                print(\"✗ No existing parent model found. Cannot proceed without parent model. Exiting.\")\n",
    "                exit(1)\n",
    "\n",
    "    # Train child models\n",
    "    results = {}\n",
    "    print(\"\\n2. Training child models sequentially...\")\n",
    "    for ticker in config.child_tickers:\n",
    "        print(f\"Training child model for {ticker}...\")\n",
    "        try:\n",
    "            summary = train_child(ticker=ticker, start=config.start_date, epochs=config.child_epochs, \n",
    "                                 parent_dir=config.parent_dir, workdir=config.workdir)\n",
    "            results[ticker] = summary\n",
    "            print(f\"✓ {ticker} model trained and saved to: {summary['checkpoint']}\")\n",
    "            print(f\"✓ Predictions saved to: {summary['json']}\")\n",
    "            print(f\"✓ Metrics saved to: {summary['checkpoint']}/{ticker}_child_metrics.json\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error training {ticker}: {e}\")\n",
    "\n",
    "    # Generate predictions\n",
    "    print(\"\\n3. Generating fresh predictions...\")\n",
    "    for ticker in config.child_tickers:\n",
    "        try:\n",
    "            preds = predict_child(ticker=ticker, parent_dir=config.parent_dir, workdir=config.workdir)\n",
    "            if \"error\" in preds:\n",
    "                print(f\"✗ Error predicting {ticker}: {preds['error']}\")\n",
    "                continue\n",
    "            predictions = preds.get('predictions', {})\n",
    "            next_business_days = predictions.get('next_business_days', [])\n",
    "            next_day = predictions.get('next_day', {})\n",
    "            next_week = predictions.get('next_week', {})\n",
    "            print(f\"✓ {ticker} predictions for {next_business_days}:\")\n",
    "            print(f\"  Next-day open: ${next_day.get('open', 'N/A'):.2f}\")\n",
    "            print(f\"  Next-day high: ${next_day.get('high', 'N/A'):.2f}\")\n",
    "            print(f\"  Next-day low: ${next_day.get('low', 'N/A'):.2f}\")\n",
    "            print(f\"  Next-day close: ${next_day.get('close', 'N/A'):.2f}\")\n",
    "            print(f\"  Next-week high: ${next_week.get('high', 'N/A'):.2f}\")\n",
    "            print(f\"  Next-week low: ${next_week.get('low', 'N/A'):.2f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error predicting {ticker}: {e}\")\n",
    "\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(\"Pipeline completed! Check 'outputs/' directory for models, scalers, predictions, metrics, and plots.\")\n",
    "    print(\"\\nFile structure:\")\n",
    "    print(\"outputs/\")\n",
    "    print(\"├── parent/\")\n",
    "    print(f\"│   ├── model.pt\")\n",
    "    print(f\"│   ├── model.onnx\")\n",
    "    print(f\"│   ├── parent_scaler.pkl\")\n",
    "    print(f\"│   └── {config.parent_ticker.replace('^', '')}_parent_metrics.json\")\n",
    "    for ticker in config.child_tickers:\n",
    "        if ticker in results:\n",
    "            print(f\"├── {ticker}/\")\n",
    "            print(f\"│   ├── model.pt\")\n",
    "            print(f\"│   ├── model.onnx\")\n",
    "            print(f\"│   ├── {ticker}_child_scaler.pkl\")\n",
    "            print(f\"│   ├── {ticker}_child_forecast.json\")\n",
    "            print(f\"│   ├── {ticker}_child_metrics.json\")\n",
    "            print(f\"│   └── {ticker}_child_history_forecast.png\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a83ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1473c5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
