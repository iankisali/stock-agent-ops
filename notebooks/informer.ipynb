{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8060d11f",
   "metadata": {},
   "source": [
    "## Entire pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5104aa",
   "metadata": {},
   "source": [
    "#### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5d7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LSTM Transfer Learning Pipeline\n",
      "==================================================\n",
      "\n",
      "1. Training parent model on ^GSPC...\n",
      "Epoch 1/8 - loss: 0.18583\n",
      "Epoch 2/8 - loss: 0.04805\n",
      "Epoch 3/8 - loss: 0.04558\n",
      "Epoch 4/8 - loss: 0.04449\n",
      "Epoch 5/8 - loss: 0.04377\n",
      "Epoch 6/8 - loss: 0.04343\n",
      "Epoch 7/8 - loss: 0.04308\n",
      "Epoch 8/8 - loss: 0.04258\n",
      "GSPC → MSE: 0.04380, RMSE: 0.20928, R²: 0.95564\n",
      "Parent model saved to: outputs/parent\n",
      "Parent metrics saved to: outputs/parent/GSPC_metrics.json\n",
      "\n",
      "2. Training child model for NVDA...\n",
      "Epoch 1/4 - loss: 0.15079\n",
      "Epoch 2/4 - loss: 0.13966\n",
      "Epoch 3/4 - loss: 0.13824\n",
      "Epoch 4/4 - loss: 0.13724\n",
      "NVDA → MSE: 0.13639, RMSE: 0.36931, R²: 0.86319\n",
      "✓ NVDA model trained and saved to: outputs/NVDA\n",
      "✓ Predictions saved to: outputs/NVDA/NVDA_forecast.json\n",
      "✓ Metrics saved to: outputs/NVDA/NVDA_metrics.json\n",
      "\n",
      "2. Training child model for AAPL...\n",
      "Epoch 1/4 - loss: 0.09208\n",
      "Epoch 2/4 - loss: 0.07641\n",
      "Epoch 3/4 - loss: 0.07476\n",
      "Epoch 4/4 - loss: 0.07445\n",
      "AAPL → MSE: 0.07428, RMSE: 0.27254, R²: 0.92609\n",
      "✓ AAPL model trained and saved to: outputs/AAPL\n",
      "✓ Predictions saved to: outputs/AAPL/AAPL_forecast.json\n",
      "✓ Metrics saved to: outputs/AAPL/AAPL_metrics.json\n",
      "\n",
      "2. Training child model for TSLA...\n",
      "Epoch 1/4 - loss: 0.10472\n",
      "Epoch 2/4 - loss: 0.10280\n",
      "Epoch 3/4 - loss: 0.10329\n",
      "Epoch 4/4 - loss: 0.10203\n",
      "TSLA → MSE: 0.10166, RMSE: 0.31884, R²: 0.89809\n",
      "✓ TSLA model trained and saved to: outputs/TSLA\n",
      "✓ Predictions saved to: outputs/TSLA/TSLA_forecast.json\n",
      "✓ Metrics saved to: outputs/TSLA/TSLA_metrics.json\n",
      "\n",
      "2. Training child model for MSFT...\n",
      "Epoch 1/4 - loss: 0.12053\n",
      "Epoch 2/4 - loss: 0.10520\n",
      "Epoch 3/4 - loss: 0.10349\n",
      "Epoch 4/4 - loss: 0.10284\n",
      "MSFT → MSE: 0.10273, RMSE: 0.32051, R²: 0.89698\n",
      "✓ MSFT model trained and saved to: outputs/MSFT\n",
      "✓ Predictions saved to: outputs/MSFT/MSFT_forecast.json\n",
      "✓ Metrics saved to: outputs/MSFT/MSFT_metrics.json\n",
      "\n",
      "3. Generating fresh predictions...\n",
      "✓ NVDA predictions:\n",
      "  Next-day open: $141.86\n",
      "  Next-day close: $141.09\n",
      "  Next-week high: $145.09\n",
      "  Next-week low: $138.44\n",
      "✓ AAPL predictions:\n",
      "  Next-day open: $226.61\n",
      "  Next-day close: $226.75\n",
      "  Next-week high: $229.48\n",
      "  Next-week low: $224.11\n",
      "✓ TSLA predictions:\n",
      "  Next-day open: $322.22\n",
      "  Next-day close: $324.37\n",
      "  Next-week high: $331.16\n",
      "  Next-week low: $314.58\n",
      "✓ MSFT predictions:\n",
      "  Next-day open: $501.03\n",
      "  Next-day close: $495.88\n",
      "  Next-week high: $504.90\n",
      "  Next-week low: $493.53\n",
      "\n",
      "==================================================\n",
      "Pipeline completed! Check 'outputs/' directory for:\n",
      "- Model checkpoints (model.pt files)\n",
      "- Scalers (scaler.pkl files)\n",
      "- Prediction JSONs (*_forecast.json)\n",
      "- Performance metrics (*_metrics.json)\n",
      "- Forecast plots (*_history_forecast.png)\n",
      "\n",
      "File structure:\n",
      "outputs/\n",
      "├── parent/\n",
      "│   ├── model.pt\n",
      "│   ├── scaler.pkl\n",
      "│   └── GSPC_metrics.json\n",
      "├── NVDA/\n",
      "│   ├── model.pt\n",
      "│   ├── scaler.pkl\n",
      "│   ├── NVDA_forecast.json\n",
      "│   ├── NVDA_metrics.json\n",
      "│   └── NVDA_history_forecast.png\n",
      "├── AAPL/\n",
      "│   ├── model.pt\n",
      "│   ├── scaler.pkl\n",
      "│   ├── AAPL_forecast.json\n",
      "│   ├── AAPL_metrics.json\n",
      "│   └── AAPL_history_forecast.png\n",
      "├── TSLA/\n",
      "│   ├── model.pt\n",
      "│   ├── scaler.pkl\n",
      "│   ├── TSLA_forecast.json\n",
      "│   ├── TSLA_metrics.json\n",
      "│   └── TSLA_history_forecast.png\n",
      "├── MSFT/\n",
      "│   ├── model.pt\n",
      "│   ├── scaler.pkl\n",
      "│   ├── MSFT_forecast.json\n",
      "│   ├── MSFT_metrics.json\n",
      "│   └── MSFT_history_forecast.png\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# LSTM pipeline with transfer learning (PyTorch)\n",
    "# ------------------------------------------------------\n",
    "# - Train parent model once on S&P 500 (^GSPC)\n",
    "# - Train child model(s) per ticker by fine-tuning parent\n",
    "# - Predict next-day Open/Close and next-week High/Low (5 trading days)\n",
    "# - Save plots, JSON, scalers, and evaluation metrics (MSE, RMSE, R²)\n",
    "\n",
    "# Quick start:\n",
    "#     pip install -U yfinance pandas numpy matplotlib torch scikit-learn joblib\n",
    "\n",
    "# Usage:\n",
    "#     parent_dir = train_parent(\"^GSPC\", start=\"2000-01-01\", epochs=8)\n",
    "#     summary = train_child(\"NVDA\", start=\"2000-01-01\", epochs=4, parent_dir=parent_dir)\n",
    "#     preds = predict_child(\"NVDA\", parent_dir=parent_dir)\n",
    "# \"\"\"\n",
    "\n",
    "# import os, json, joblib\n",
    "# from typing import Dict\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import yfinance as yf\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# CONTEXT_LEN = 60  # lookback days\n",
    "# PRED_LEN = 5      # forecast horizon (days)\n",
    "# INPUT_SIZE = 5    # OHLCV\n",
    "# BATCH_SIZE = 32\n",
    "\n",
    "# # -----------------------------\n",
    "# # Data utilities\n",
    "# # -----------------------------\n",
    "\n",
    "# def fetch_ohlcv(ticker: str, start: str = \"2000-01-01\", end: str | None = None) -> pd.DataFrame:\n",
    "#     df = yf.download(ticker, start=start, end=end, interval=\"1d\", auto_adjust=True, progress=False)\n",
    "#     df = df.reset_index().rename(columns={\"Date\": \"date\"})\n",
    "#     df = df[[\"date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].dropna()\n",
    "#     return df\n",
    "\n",
    "\n",
    "# class StockDataset(Dataset):\n",
    "#     def __init__(self, df: pd.DataFrame, scaler: StandardScaler, context_len=CONTEXT_LEN, pred_len=PRED_LEN):\n",
    "#         vals = scaler.transform(df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]])\n",
    "#         vals = vals.astype(\"float32\")\n",
    "\n",
    "#         self.samples = []\n",
    "#         for t in range(context_len, len(df) - pred_len):\n",
    "#             past = vals[t - context_len:t]\n",
    "#             fut = vals[t:t + pred_len]\n",
    "#             self.samples.append((past, fut))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.samples)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         past, fut = self.samples[idx]\n",
    "#         return torch.tensor(past), torch.tensor(fut)\n",
    "\n",
    "\n",
    "# # -----------------------------\n",
    "# # LSTM Model\n",
    "# # -----------------------------\n",
    "\n",
    "# class LSTMModel(nn.Module):\n",
    "#     def __init__(self, input_size=INPUT_SIZE, hidden_size=64, num_layers=2, pred_len=PRED_LEN):\n",
    "#         super().__init__()\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, input_size * pred_len)\n",
    "#         self.pred_len = pred_len\n",
    "#         self.input_size = input_size\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out, _ = self.lstm(x)\n",
    "#         out = out[:, -1, :]  # last hidden state\n",
    "#         out = self.fc(out)\n",
    "#         out = out.view(-1, self.pred_len, self.input_size)\n",
    "#         return out\n",
    "\n",
    "\n",
    "# # -----------------------------\n",
    "# # Training\n",
    "# # -----------------------------\n",
    "\n",
    "# def fit_model(model: nn.Module, loader: DataLoader, epochs=8, lr=1e-3):\n",
    "#     model.to(DEVICE)\n",
    "#     opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#     criterion = nn.MSELoss()\n",
    "\n",
    "#     for ep in range(1, epochs + 1):\n",
    "#         model.train()\n",
    "#         total_loss = 0.0\n",
    "#         for X, Y in loader:\n",
    "#             X, Y = X.to(DEVICE), Y.to(DEVICE)\n",
    "#             opt.zero_grad()\n",
    "#             pred = model(X)\n",
    "#             loss = criterion(pred, Y)\n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#             opt.step()\n",
    "#             total_loss += loss.item()\n",
    "#         avg = total_loss / len(loader)\n",
    "#         print(f\"Epoch {ep}/{epochs} - loss: {avg:.5f}\")\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def save_model(model: nn.Module, scaler: StandardScaler, path: str):\n",
    "#     os.makedirs(path, exist_ok=True)\n",
    "#     torch.save(model.state_dict(), os.path.join(path, \"model.pt\"))\n",
    "#     joblib.dump(scaler, os.path.join(path, \"scaler.pkl\"))\n",
    "\n",
    "\n",
    "# def load_model(path: str) -> tuple[LSTMModel, StandardScaler]:\n",
    "#     model = LSTMModel()\n",
    "#     model.load_state_dict(torch.load(os.path.join(path, \"model.pt\"), map_location=DEVICE))\n",
    "#     scaler = joblib.load(os.path.join(path, \"scaler.pkl\"))\n",
    "#     model.to(DEVICE)\n",
    "#     return model, scaler\n",
    "\n",
    "# # -----------------------------\n",
    "# # Inference & Evaluation\n",
    "# # -----------------------------\n",
    "\n",
    "# def predict_one_step_and_week(model: nn.Module, df: pd.DataFrame, scaler: StandardScaler) -> Dict:\n",
    "#     model.eval()\n",
    "#     vals = scaler.transform(df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]).astype(\"float32\")\n",
    "#     X = torch.tensor(vals[-CONTEXT_LEN:]).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         pred = model(X).squeeze(0).cpu().numpy()\n",
    "#     pred = scaler.inverse_transform(pred)\n",
    "\n",
    "#     next_day_open = float(pred[0, 0])\n",
    "#     next_day_close = float(pred[0, 3])\n",
    "#     next_week_high = float(np.max(pred[:, 1]))\n",
    "#     next_week_low = float(np.min(pred[:, 2]))\n",
    "\n",
    "#     last_date = pd.to_datetime(df[\"date\"].iloc[-1])\n",
    "#     future_dates = pd.bdate_range(last_date + pd.Timedelta(days=1), periods=PRED_LEN)\n",
    "\n",
    "#     payload = {\n",
    "#         \"last_date\": str(last_date.date()),\n",
    "#         \"future_window_days\": int(PRED_LEN),\n",
    "#         \"predictions\": {\n",
    "#             \"next_day_open\": next_day_open,\n",
    "#             \"next_day_close\": next_day_close,\n",
    "#             \"next_week_high\": next_week_high,\n",
    "#             \"next_week_low\": next_week_low,\n",
    "#         },\n",
    "#         \"daily_trend\": [\n",
    "#             {\n",
    "#                 \"date\": str(d.date()),\n",
    "#                 \"open\": float(pred[i, 0]),\n",
    "#                 \"high\": float(pred[i, 1]),\n",
    "#                 \"low\": float(pred[i, 2]),\n",
    "#                 \"close\": float(pred[i, 3]),\n",
    "#             }\n",
    "#             for i, d in enumerate(future_dates)\n",
    "#         ],\n",
    "#     }\n",
    "#     return payload\n",
    "\n",
    "\n",
    "# def evaluate_model(model: nn.Module, df: pd.DataFrame, scaler: StandardScaler, out_dir: str, ticker: str) -> Dict:\n",
    "#     model.eval()\n",
    "#     vals = scaler.transform(df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]).astype(\"float32\")\n",
    "#     X, Y = [], []\n",
    "#     for t in range(CONTEXT_LEN, len(vals) - PRED_LEN):\n",
    "#         X.append(vals[t - CONTEXT_LEN:t])\n",
    "#         Y.append(vals[t:t + PRED_LEN])\n",
    "#     X, Y = np.array(X), np.array(Y)\n",
    "\n",
    "#     if len(X) == 0:\n",
    "#         return {}\n",
    "\n",
    "#     X_t = torch.tensor(X).to(DEVICE)\n",
    "#     with torch.no_grad():\n",
    "#         preds = model(X_t).cpu().numpy()\n",
    "\n",
    "#     mse = mean_squared_error(Y.reshape(-1, INPUT_SIZE), preds.reshape(-1, INPUT_SIZE))\n",
    "#     rmse = np.sqrt(mse)\n",
    "#     r2 = r2_score(Y.reshape(-1, INPUT_SIZE), preds.reshape(-1, INPUT_SIZE))\n",
    "\n",
    "#     metrics = {\"MSE\": mse, \"RMSE\": rmse, \"R2\": r2}\n",
    "#     with open(os.path.join(out_dir, f\"{ticker}_metrics.json\"), \"w\") as f:\n",
    "#         json.dump(metrics, f, indent=2)\n",
    "#     print(f\"{ticker} → MSE: {mse:.5f}, RMSE: {rmse:.5f}, R²: {r2:.5f}\")\n",
    "#     return metrics\n",
    "\n",
    "\n",
    "# def save_json(payload: Dict, path: str):\n",
    "#     os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "#     with open(path, \"w\") as f:\n",
    "#         json.dump(payload, f, indent=2)\n",
    "#     return path\n",
    "\n",
    "\n",
    "# def plot_outputs(df: pd.DataFrame, payload: Dict, out_dir: str, ticker: str):\n",
    "#     os.makedirs(out_dir, exist_ok=True)\n",
    "#     plt.figure(figsize=(12, 5))\n",
    "#     plt.plot(df[\"date\"], df[\"Close\"], label=\"History\")\n",
    "#     ndc = payload[\"predictions\"][\"next_day_close\"]\n",
    "#     whi = payload[\"predictions\"][\"next_week_high\"]\n",
    "#     wlo = payload[\"predictions\"][\"next_week_low\"]\n",
    "#     plt.axhline(ndc, color=\"r\", linestyle=\"--\", label=\"Next-day close\")\n",
    "#     plt.axhline(whi, color=\"g\", linestyle=\":\", label=\"Next-week high\")\n",
    "#     plt.axhline(wlo, color=\"b\", linestyle=\":\", label=\"Next-week low\")\n",
    "#     plt.legend()\n",
    "#     plt.title(f\"{ticker} Close + Forecast\")\n",
    "#     plt.savefig(os.path.join(out_dir, f\"{ticker}_history_forecast.png\"))\n",
    "#     plt.close()\n",
    "\n",
    "# # -----------------------------\n",
    "# # Public functions\n",
    "# # -----------------------------\n",
    "\n",
    "# def train_parent(parent_ticker=\"^GSPC\", start=\"2000-01-01\", epochs=8, out_dir=\"outputs/parent\") -> str:\n",
    "#     df = fetch_ohlcv(parent_ticker, start=start)\n",
    "#     scaler = StandardScaler().fit(df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]])\n",
    "#     dataset = StockDataset(df, scaler)\n",
    "#     loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "#     model = LSTMModel()\n",
    "#     model = fit_model(model, loader, epochs=epochs, lr=1e-3)\n",
    "#     save_model(model, scaler, out_dir)\n",
    "#     evaluate_model(model, df, scaler, out_dir, parent_ticker.replace(\"^\", \"\"))\n",
    "#     return out_dir\n",
    "\n",
    "\n",
    "# def train_child(child_ticker: str, start=\"2000-01-01\", epochs=4, parent_dir=\"outputs/parent\", workdir=\"outputs\") -> Dict:\n",
    "#     df = fetch_ohlcv(child_ticker, start=start)\n",
    "#     parent_model, _ = load_model(parent_dir)\n",
    "\n",
    "#     # Freeze lower LSTM layers for transfer learning\n",
    "#     for name, param in parent_model.named_parameters():\n",
    "#         if \"lstm\" in name:\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#     scaler = StandardScaler().fit(df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]])\n",
    "#     dataset = StockDataset(df, scaler)\n",
    "#     loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "#     child_model = fit_model(parent_model, loader, epochs=epochs, lr=3e-4)\n",
    "#     child_dir = os.path.join(workdir, child_ticker)\n",
    "#     save_model(child_model, scaler, child_dir)\n",
    "\n",
    "#     payload = predict_one_step_and_week(child_model, df, scaler)\n",
    "#     json_path = os.path.join(child_dir, f\"{child_ticker}_forecast.json\")\n",
    "#     save_json(payload, json_path)\n",
    "#     plot_outputs(df, payload, child_dir, child_ticker)\n",
    "#     evaluate_model(child_model, df, scaler, child_dir, child_ticker)\n",
    "#     return {\"checkpoint\": child_dir, \"json\": json_path}\n",
    "\n",
    "\n",
    "# def predict_child(child_ticker: str, parent_dir=\"outputs/parent\", workdir=\"outputs\") -> Dict:\n",
    "#     child_dir = os.path.join(workdir, child_ticker)\n",
    "#     df = fetch_ohlcv(child_ticker, start=\"2000-01-01\")\n",
    "#     model, scaler = load_model(child_dir)\n",
    "#     payload = predict_one_step_and_week(model, df, scaler)\n",
    "#     return payload\n",
    "\n",
    "\n",
    "# # -----------------------------\n",
    "# # Main execution\n",
    "# # -----------------------------\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # Configuration\n",
    "#     PARENT_TICKER = \"^GSPC\"  # S&P 500 as parent\n",
    "#     CHILD_TICKERS = [\"NVDA\", \"AAPL\", \"TSLA\", \"MSFT\"]  # Example child stocks\n",
    "#     START_DATE = \"2000-01-01\"\n",
    "#     PARENT_EPOCHS = 8\n",
    "#     CHILD_EPOCHS = 4\n",
    "    \n",
    "#     print(\"Starting LSTM Transfer Learning Pipeline\")\n",
    "#     print(\"=\" * 50)\n",
    "    \n",
    "#     # Step 1: Train parent model on S&P 500\n",
    "#     print(f\"\\n1. Training parent model on {PARENT_TICKER}...\")\n",
    "#     parent_dir = train_parent(\n",
    "#         parent_ticker=PARENT_TICKER,\n",
    "#         start=START_DATE,\n",
    "#         epochs=PARENT_EPOCHS,\n",
    "#         out_dir=\"outputs/parent\"\n",
    "#     )\n",
    "#     print(f\"Parent model saved to: {parent_dir}\")\n",
    "#     print(f\"Parent metrics saved to: {parent_dir}/{PARENT_TICKER.replace('^', '')}_metrics.json\")\n",
    "    \n",
    "#     # Step 2: Train child models with transfer learning\n",
    "#     results = {}\n",
    "#     for ticker in CHILD_TICKERS:\n",
    "#         print(f\"\\n2. Training child model for {ticker}...\")\n",
    "#         try:\n",
    "#             summary = train_child(\n",
    "#                 child_ticker=ticker,\n",
    "#                 start=START_DATE,\n",
    "#                 epochs=CHILD_EPOCHS,\n",
    "#                 parent_dir=parent_dir,\n",
    "#                 workdir=\"outputs\"\n",
    "#             )\n",
    "#             results[ticker] = summary\n",
    "#             print(f\"✓ {ticker} model trained and saved to: {summary['checkpoint']}\")\n",
    "#             print(f\"✓ Predictions saved to: {summary['json']}\")\n",
    "#             print(f\"✓ Metrics saved to: {summary['checkpoint']}/{ticker}_metrics.json\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"✗ Error training {ticker}: {e}\")\n",
    "#             continue\n",
    "    \n",
    "#     # Step 3: Generate fresh predictions\n",
    "#     print(f\"\\n3. Generating fresh predictions...\")\n",
    "#     for ticker in CHILD_TICKERS:\n",
    "#         if ticker in results:\n",
    "#             try:\n",
    "#                 preds = predict_child(ticker, parent_dir=parent_dir, workdir=\"outputs\")\n",
    "#                 print(f\"✓ {ticker} predictions:\")\n",
    "#                 print(f\"  Next-day open: ${preds['predictions']['next_day_open']:.2f}\")\n",
    "#                 print(f\"  Next-day close: ${preds['predictions']['next_day_close']:.2f}\")\n",
    "#                 print(f\"  Next-week high: ${preds['predictions']['next_week_high']:.2f}\")\n",
    "#                 print(f\"  Next-week low: ${preds['predictions']['next_week_low']:.2f}\")\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 print(f\"✗ Error predicting {ticker}: {e}\")\n",
    "    \n",
    "#     print(f\"\\n\" + \"=\" * 50)\n",
    "#     print(\"Pipeline completed! Check 'outputs/' directory for:\")\n",
    "#     print(\"- Model checkpoints (model.pt files)\")\n",
    "#     print(\"- Scalers (scaler.pkl files)\")  \n",
    "#     print(\"- Prediction JSONs (*_forecast.json)\")\n",
    "#     print(\"- Performance metrics (*_metrics.json)\")\n",
    "#     print(\"- Forecast plots (*_history_forecast.png)\")\n",
    "#     print(\"\\nFile structure:\")\n",
    "#     print(\"outputs/\")\n",
    "#     print(\"├── parent/\")\n",
    "#     print(f\"│   ├── model.pt\")\n",
    "#     print(f\"│   ├── scaler.pkl\")\n",
    "#     print(f\"│   └── {PARENT_TICKER.replace('^', '')}_metrics.json\")\n",
    "#     for ticker in CHILD_TICKERS:\n",
    "#         if ticker in results:\n",
    "#             print(f\"├── {ticker}/\")\n",
    "#             print(f\"│   ├── model.pt\")\n",
    "#             print(f\"│   ├── scaler.pkl\") \n",
    "#             print(f\"│   ├── {ticker}_forecast.json\")\n",
    "#             print(f\"│   ├── {ticker}_metrics.json\")\n",
    "#             print(f\"│   └── {ticker}_history_forecast.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4648a4",
   "metadata": {},
   "source": [
    "### Each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0030ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from typing import Dict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import ta\n",
    "\n",
    "from comet_ml import Experiment\n",
    "from comet_ml.integration.pytorch import log_model\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CONTEXT_LEN = 60  # lookback days\n",
    "PRED_LEN = 1     # forecast horizon (days)\n",
    "INPUT_SIZE = 5    # OHLCV\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Set environment variables\n",
    "load_dotenv()\n",
    "os.environ[\"COMET_API_KEY\"] = os.getenv(\"COMET_API_KEY\")\n",
    "\n",
    "# -----------------------------\n",
    "# Data utilities\n",
    "# -----------------------------\n",
    "\n",
    "def fetch_ohlcv(ticker: str, start: str = \"2000-01-01\", end: str | None = None) -> pd.DataFrame:\n",
    "    df = yf.download(ticker, start=start, end=end, interval=\"1d\", auto_adjust=True, progress=False)\n",
    "    df = df.reset_index().rename(columns={\"Date\": \"date\"})\n",
    "    df = df[[\"date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].dropna()\n",
    "    return df\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, scaler: StandardScaler, context_len=CONTEXT_LEN, pred_len=PRED_LEN):\n",
    "        vals = scaler.transform(df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]])\n",
    "        vals = vals.astype(\"float32\")\n",
    "        self.samples = []\n",
    "        for t in range(context_len, len(df) - pred_len):\n",
    "            past = vals[t - context_len:t]\n",
    "            fut = vals[t:t + pred_len]\n",
    "            self.samples.append((past, fut))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        past, fut = self.samples[idx]\n",
    "        return torch.tensor(past), torch.tensor(fut)\n",
    "\n",
    "# -----------------------------\n",
    "# LSTM Model\n",
    "# -----------------------------\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=INPUT_SIZE, hidden_size=128, num_layers=3, pred_len=PRED_LEN, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, input_size * pred_len)\n",
    "        self.pred_len = pred_len\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # Last hidden state\n",
    "        out = self.fc(out)\n",
    "        out = out.view(-1, self.pred_len, self.input_size)\n",
    "        return out\n",
    "\n",
    "# -----------------------------\n",
    "# Training\n",
    "# -----------------------------\n",
    "\n",
    "def fit_model(model: nn.Module, loader: DataLoader, val_loader: DataLoader, epochs=8, lr=1e-3, experiment: Experiment = None):\n",
    "    model.to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=3)\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    counter = 0\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for X, Y in loader:\n",
    "            X, Y = X.to(DEVICE), Y.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, Y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_train_loss = total_loss / len(loader)\n",
    "        print(f\"Epoch {ep}/{epochs} - Train Loss: {avg_train_loss:.5f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X, Y in val_loader:\n",
    "                X, Y = X.to(DEVICE), Y.to(DEVICE)\n",
    "                pred = model(X)\n",
    "                val_loss += criterion(pred, Y).item()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {ep}/{epochs} - Val Loss: {avg_val_loss:.5f}\")\n",
    "\n",
    "        if experiment:\n",
    "            experiment.log_metric(\"train_loss\", avg_train_loss, epoch=ep)\n",
    "            experiment.log_metric(\"val_loss\", avg_val_loss, epoch=ep)\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    return model\n",
    "\n",
    "def save_model(model: nn.Module, scaler: StandardScaler, path: str, experiment: Experiment = None, model_type: str = \"parent\", ticker: str = None):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    # Save locally\n",
    "    torch.save(model.state_dict(), os.path.join(path, \"model.pt\"))\n",
    "    scaler_filename = \"parent_scaler.pkl\" if model_type == \"parent\" else f\"{ticker}_child_scaler.pkl\"\n",
    "    joblib.dump(scaler, os.path.join(path, scaler_filename))\n",
    "    \n",
    "    if experiment:\n",
    "        model_name = \"lstm_parent_model_checkpoint\" if model_type == \"parent\" else f\"lstm_child_model_checkpoint_{ticker}\"\n",
    "        checkpoint = {\"model_state_dict\": model.state_dict()}\n",
    "        log_model(\n",
    "            experiment=experiment,\n",
    "            model=checkpoint,\n",
    "            model_name=model_name,\n",
    "            metadata={\n",
    "                \"input_size\": INPUT_SIZE,\n",
    "                \"context_len\": CONTEXT_LEN,\n",
    "                \"pred_len\": PRED_LEN,\n",
    "                \"model_type\": model_type,\n",
    "                \"ticker\": ticker or \"SP500\",\n",
    "                \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "            }\n",
    "        )\n",
    "        experiment.log_asset(os.path.join(path, scaler_filename), file_name=scaler_filename)\n",
    "\n",
    "def load_model(path: str = None, experiment_key: str = None, model_type: str = \"parent\", ticker: str = None) -> tuple[LSTMModel, StandardScaler]:\n",
    "    if model_type == \"child\" and not ticker:\n",
    "        raise ValueError(\"Ticker must be provided for child model\")\n",
    "    \n",
    "    model = LSTMModel().to(DEVICE)\n",
    "    scaler = None\n",
    "    model_name = \"lstm_parent_model_checkpoint\" if model_type == \"parent\" else f\"lstm_child_model_checkpoint_{ticker}\"\n",
    "    scaler_filename = \"parent_scaler.pkl\" if model_type == \"parent\" else f\"{ticker}_child_scaler.pkl\"\n",
    "    project_name = \"S&P-500-parent-model\" if model_type == \"parent\" else \"child-model\"\n",
    "\n",
    "    if experiment_key:\n",
    "        experiment = Experiment(api_key=os.getenv(\"COMET_API_KEY\"), project_name=project_name)\n",
    "        experiment.set_experiment_key(experiment_key)\n",
    "        \n",
    "        checkpoint = load_model(f\"experiment://{experiment_key}/{model_name}\")\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        \n",
    "        asset_list = experiment.get_asset_list()\n",
    "        scaler_asset_id = None\n",
    "        for asset in asset_list:\n",
    "            if asset[\"fileName\"] == scaler_filename:\n",
    "                scaler_asset_id = asset[\"assetId\"]\n",
    "                break\n",
    "        if scaler_asset_id:\n",
    "            scaler_data = experiment.get_asset(scaler_asset_id, return_type=\"binary\")\n",
    "            with open(f\"temp_{scaler_filename}\", \"wb\") as f:\n",
    "                f.write(scaler_data)\n",
    "            scaler = joblib.load(f\"temp_{scaler_filename}\")\n",
    "            os.remove(f\"temp_{scaler_filename}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Scaler asset '{scaler_filename}' not found in Comet ML experiment\")\n",
    "        \n",
    "        experiment.end()\n",
    "    elif path:\n",
    "        model.load_state_dict(torch.load(os.path.join(path, \"model.pt\"), map_location=DEVICE))\n",
    "        scaler = joblib.load(os.path.join(path, scaler_filename))\n",
    "    else:\n",
    "        raise ValueError(\"Must provide either path or experiment_key\")\n",
    "\n",
    "    model.eval()\n",
    "    return model, scaler\n",
    "\n",
    "# -----------------------------\n",
    "# Inference & Evaluation\n",
    "# -----------------------------\n",
    "\n",
    "def predict_one_step_and_week(model: nn.Module, df: pd.DataFrame, scaler: StandardScaler) -> Dict:\n",
    "    model.eval()\n",
    "    vals = scaler.transform(df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]).astype(\"float32\")\n",
    "    X = torch.tensor(vals[-CONTEXT_LEN:]).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(X).squeeze(0).cpu().numpy()\n",
    "    pred = scaler.inverse_transform(pred)\n",
    "\n",
    "    next_day_open = float(pred[0, 0])\n",
    "    next_day_high = float(pred[0, 1])\n",
    "    next_day_low = float(pred[0, 2])\n",
    "    next_day_close = float(pred[0, 3])\n",
    "    \n",
    "    next_week_high = float(np.max(pred[:, 1]))\n",
    "    next_week_low = float(np.min(pred[:, 2]))\n",
    "\n",
    "    last_date = pd.to_datetime(df[\"date\"].iloc[-1])\n",
    "    next_business_day = pd.bdate_range(last_date + pd.Timedelta(days=1), periods=1)[0]\n",
    "\n",
    "    payload = {\n",
    "        \"last_date\": str(last_date.date()),\n",
    "        \"next_business_day\": str(next_business_day.date()),\n",
    "        \"future_window_days\": int(PRED_LEN),\n",
    "        \"predictions\": {\n",
    "            \"next_day_open\": next_day_open,\n",
    "            \"next_day_high\": next_day_high,\n",
    "            \"next_day_low\": next_day_low,\n",
    "            \"next_day_close\": next_day_close,\n",
    "            \"next_week_high\": next_week_high,\n",
    "            \"next_week_low\": next_week_low,\n",
    "        }\n",
    "    }\n",
    "    return payload\n",
    "\n",
    "def evaluate_model(model: nn.Module, df: pd.DataFrame, scaler: StandardScaler, out_dir: str, ticker: str, experiment: Experiment = None) -> Dict:\n",
    "    model.eval()\n",
    "    vals = scaler.transform(df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]).astype(\"float32\")\n",
    "    X, Y = [], []\n",
    "    for t in range(CONTEXT_LEN, len(vals) - PRED_LEN):\n",
    "        X.append(vals[t - CONTEXT_LEN:t])\n",
    "        Y.append(vals[t:t + PRED_LEN])\n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "\n",
    "    if len(X) == 0:\n",
    "        return {}\n",
    "\n",
    "    X_t = torch.tensor(X).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_t).cpu().numpy()\n",
    "\n",
    "    mse = mean_squared_error(Y.reshape(-1, INPUT_SIZE), preds.reshape(-1, INPUT_SIZE))\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(Y.reshape(-1, INPUT_SIZE), preds.reshape(-1, INPUT_SIZE))\n",
    "\n",
    "    metrics = {\"MSE\": mse, \"RMSE\": rmse, \"R2\": r2}\n",
    "    metrics_filename = f\"{ticker}_parent_metrics.json\" if \"parent\" in out_dir else f\"{ticker}_child_metrics.json\"\n",
    "    with open(os.path.join(out_dir, metrics_filename), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"{ticker} → MSE: {mse:.5f}, RMSE: {rmse:.5f}, R²: {r2:.5f}\")\n",
    "    if experiment:\n",
    "        experiment.log_metrics({\"mse\": mse, \"rmse\": rmse, \"r2\": r2})\n",
    "        experiment.log_asset(os.path.join(out_dir, metrics_filename), file_name=metrics_filename)\n",
    "    return metrics\n",
    "\n",
    "def save_json(payload: Dict, path: str):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(payload, f, indent=2)\n",
    "    return path\n",
    "\n",
    "def plot_outputs(df: pd.DataFrame, payload: Dict, out_dir: str, ticker: str, experiment: Experiment = None):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(df[\"date\"], df[\"Close\"], label=\"History\")\n",
    "    \n",
    "    ndo = payload[\"predictions\"][\"next_day_open\"]\n",
    "    ndc = payload[\"predictions\"][\"next_day_close\"]\n",
    "    ndh = payload[\"predictions\"][\"next_day_high\"]\n",
    "    ndl = payload[\"predictions\"][\"next_day_low\"]\n",
    "    whi = payload[\"predictions\"][\"next_week_high\"]\n",
    "    wlo = payload[\"predictions\"][\"next_week_low\"]\n",
    "    \n",
    "    plt.axhline(ndo, color=\"orange\", linestyle=\"-\", alpha=0.7, label=\"Next-day open\")\n",
    "    plt.axhline(ndc, color=\"r\", linestyle=\"--\", label=\"Next-day close\")\n",
    "    plt.axhline(ndh, color=\"darkgreen\", linestyle=\"-\", alpha=0.7, label=\"Next-day high\")\n",
    "    plt.axhline(ndl, color=\"darkred\", linestyle=\"-\", alpha=0.7, label=\"Next-day low\")\n",
    "    plt.axhline(whi, color=\"g\", linestyle=\":\", label=\"Next-week high\")\n",
    "    plt.axhline(wlo, color=\"b\", linestyle=\":\", label=\"Next-week low\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"{ticker} Close + Next Day & Week Forecast\")\n",
    "    plot_filename = f\"{ticker}_parent_history_forecast.png\" if \"parent\" in out_dir else f\"{ticker}_child_history_forecast.png\"\n",
    "    plot_path = os.path.join(out_dir, plot_filename)\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    if experiment:\n",
    "        experiment.log_image(plot_path, name=plot_filename)\n",
    "\n",
    "# -----------------------------\n",
    "# Public functions\n",
    "# -----------------------------\n",
    "\n",
    "def train_parent(parent_ticker=\"^GSPC\", start=\"2000-01-01\", epochs=20, out_dir=\"outputs/parent\"):\n",
    "    experiment = Experiment(project_name=\"S&P-500-parent-model\", auto_metric_logging=False)\n",
    "    experiment.set_name(f\"parent_{parent_ticker.replace('^', '')}\")\n",
    "    experiment.add_tag(\"parent\")\n",
    "    experiment.log_parameters({\n",
    "        \"ticker\": parent_ticker,\n",
    "        \"start\": start,\n",
    "        \"epochs\": epochs,\n",
    "        \"lr\": 1e-3,\n",
    "        \"hidden_size\": 128,\n",
    "        \"num_layers\": 3,\n",
    "        \"dropout\": 0.2,\n",
    "        \"context_len\": CONTEXT_LEN,\n",
    "        \"pred_len\": PRED_LEN,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"input_size\": INPUT_SIZE\n",
    "    })\n",
    "\n",
    "    df = fetch_ohlcv(parent_ticker, start=start)\n",
    "    scaler = StandardScaler().fit(df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]])\n",
    "    dataset = StockDataset(df, scaler)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = LSTMModel()\n",
    "    model = fit_model(model, train_loader, val_loader, epochs=epochs, lr=1e-3, experiment=experiment)\n",
    "    save_model(model, scaler, out_dir, experiment=experiment, model_type=\"parent\", ticker=parent_ticker)\n",
    "    evaluate_model(model, df, scaler, out_dir, parent_ticker.replace(\"^\", \"\"), experiment=experiment)\n",
    "    experiment_key = experiment.get_key()\n",
    "    experiment.end()\n",
    "    return {\"checkpoint\": out_dir, \"experiment_key\": experiment_key}\n",
    "\n",
    "def train_child(child_ticker: str, start=\"2000-01-01\", epochs=4, parent_dir=\"outputs/parent\", workdir=\"outputs\") -> Dict:\n",
    "    experiment = Experiment(project_name=\"child-model\", auto_metric_logging=False)\n",
    "    experiment.set_name(f\"child_{child_ticker}\")\n",
    "    experiment.add_tag(f\"child-{child_ticker}\")\n",
    "    experiment.log_parameters({\n",
    "        \"ticker\": child_ticker,\n",
    "        \"start\": start,\n",
    "        \"epochs\": epochs,\n",
    "        \"lr\": 3e-4,\n",
    "        \"hidden_size\": 64,\n",
    "        \"num_layers\": 2,\n",
    "        \"context_len\": CONTEXT_LEN,\n",
    "        \"pred_len\": PRED_LEN,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"input_size\": INPUT_SIZE,\n",
    "        \"parent_dir\": parent_dir\n",
    "    })\n",
    "\n",
    "    df = fetch_ohlcv(child_ticker, start=start)\n",
    "    parent_model, _ = load_model(path=parent_dir, model_type=\"parent\")\n",
    "\n",
    "    for name, param in parent_model.named_parameters():\n",
    "        if \"lstm\" in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    scaler = StandardScaler().fit(df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]])\n",
    "    dataset = StockDataset(df, scaler)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    child_model = fit_model(parent_model, train_loader, val_loader, epochs=epochs, lr=3e-4, experiment=experiment)\n",
    "    child_dir = os.path.join(workdir, child_ticker)\n",
    "    save_model(child_model, scaler, child_dir, experiment=experiment, model_type=\"child\", ticker=child_ticker)\n",
    "\n",
    "    payload = predict_one_step_and_week(child_model, df, scaler)\n",
    "    json_filename = f\"{child_ticker}_child_forecast.json\"\n",
    "    json_path = os.path.join(child_dir, json_filename)\n",
    "    save_json(payload, json_path)\n",
    "    if experiment:\n",
    "        experiment.log_asset(json_path, file_name=json_filename)\n",
    "        experiment.log_metrics(payload[\"predictions\"])\n",
    "    plot_outputs(df, payload, child_dir, child_ticker, experiment=experiment)\n",
    "    evaluate_model(child_model, df, scaler, child_dir, child_ticker, experiment=experiment)\n",
    "    experiment_key = experiment.get_key()\n",
    "    experiment.end()\n",
    "    return {\"checkpoint\": child_dir, \"json\": json_path, \"experiment_key\": experiment_key}\n",
    "\n",
    "def predict_child(child_ticker: str, parent_dir=\"outputs/parent\", workdir=\"outputs\", experiment_key: str = None) -> Dict:\n",
    "    child_dir = os.path.join(workdir, child_ticker)\n",
    "    df = fetch_ohlcv(child_ticker, start=\"2000-01-01\")\n",
    "    model, scaler = load_model(path=child_dir, experiment_key=experiment_key, model_type=\"child\", ticker=child_ticker)\n",
    "    payload = predict_one_step_and_week(model, df, scaler)\n",
    "    return payload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "119114d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "PARENT_TICKER = \"^GSPC\"  # S&P 500 as parent\n",
    "CHILD_TICKERS = [\"GOOG\", \"AMZN\", \"META\", \"AXP\"]  # Example child stocks\n",
    "START_DATE = \"2000-01-01\"\n",
    "PARENT_EPOCHS = 20\n",
    "CHILD_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8b426fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Training parent model for S&P 500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/karan-shingde/s-p-500-parent-model/e9f64eb655194739abeb81f0fbe2a005\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/Users/karan/Documents/machine-learning/everything-mlops/mlops-from-scrstch' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8 - Train Loss: 0.13326\n",
      "Epoch 1/8 - Val Loss: 0.05154\n",
      "Epoch 2/8 - Train Loss: 0.04729\n",
      "Epoch 2/8 - Val Loss: 0.04179\n",
      "Epoch 3/8 - Train Loss: 0.04145\n",
      "Epoch 3/8 - Val Loss: 0.04001\n",
      "Epoch 4/8 - Train Loss: 0.04096\n",
      "Epoch 4/8 - Val Loss: 0.04411\n",
      "Epoch 5/8 - Train Loss: 0.03804\n",
      "Epoch 5/8 - Val Loss: 0.03645\n",
      "Epoch 6/8 - Train Loss: 0.03770\n",
      "Epoch 6/8 - Val Loss: 0.03966\n",
      "Epoch 7/8 - Train Loss: 0.03752\n",
      "Epoch 7/8 - Val Loss: 0.03913\n",
      "Epoch 8/8 - Train Loss: 0.03660\n",
      "Epoch 8/8 - Val Loss: 0.03649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : parent_GSPC\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/karan-shingde/s-p-500-parent-model/e9f64eb655194739abeb81f0fbe2a005\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mse            : 0.035174526274204254\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     r2             : 0.9644085764884949\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     rmse           : 0.18754873039880662\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss [8] : (0.0365980292117456, 0.1332612611935474)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_loss [8]   : (0.036445311200805006, 0.05154292124789208)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : parent_GSPC\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size  : 32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     context_len : 60\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout     : 0.2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs      : 8\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hidden_size : 128\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     input_size  : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr          : 0.001\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_layers  : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pred_len    : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     start       : 2000-01-01\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     ticker      : ^GSPC\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     asset                        : 2 (811 bytes)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model-element                : 2 (1.28 MB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSPC → MSE: 0.03517, RMSE: 0.18755, R²: 0.96441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Parent model trained and saved to: outputs/parent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    PARENT_TICKER = \"^GSPC\"\n",
    "    CHILD_TICKERS = [\"NVDA\", \"AAPL\"]\n",
    "    START_DATE = \"2000-01-01\"\n",
    "    CHILD_EPOCHS = 4\n",
    "\n",
    "    # Step 1: Train parent model\n",
    "    print(\"1. Training parent model for S&P 500...\")\n",
    "    parent_summary = train_parent(PARENT_TICKER, start=START_DATE, epochs=8)\n",
    "    parent_dir = parent_summary[\"checkpoint\"]\n",
    "    print(f\"✓ Parent model trained and saved to: {parent_dir}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df594e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Training child model for GOOG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/karan-shingde/child-model/10af895e8f26430fbc662a2445492868\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/Users/karan/Documents/machine-learning/everything-mlops/mlops-from-scrstch' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 0.05583\n",
      "Epoch 1/10 - Val Loss: 0.04103\n",
      "Epoch 2/10 - Train Loss: 0.04944\n",
      "Epoch 2/10 - Val Loss: 0.03999\n",
      "Epoch 3/10 - Train Loss: 0.04934\n",
      "Epoch 3/10 - Val Loss: 0.03963\n",
      "Epoch 4/10 - Train Loss: 0.04823\n",
      "Epoch 4/10 - Val Loss: 0.03938\n",
      "Epoch 5/10 - Train Loss: 0.04800\n",
      "Epoch 5/10 - Val Loss: 0.03938\n",
      "Epoch 6/10 - Train Loss: 0.04869\n",
      "Epoch 6/10 - Val Loss: 0.03933\n",
      "Epoch 7/10 - Train Loss: 0.04922\n",
      "Epoch 7/10 - Val Loss: 0.03935\n",
      "Epoch 8/10 - Train Loss: 0.04788\n",
      "Epoch 8/10 - Val Loss: 0.03944\n",
      "Epoch 9/10 - Train Loss: 0.04817\n",
      "Epoch 9/10 - Val Loss: 0.03916\n",
      "Epoch 10/10 - Train Loss: 0.04859\n",
      "Epoch 10/10 - Val Loss: 0.03936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : child_GOOG\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/karan-shingde/child-model/10af895e8f26430fbc662a2445492868\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mse             : 0.045351333916187286\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_day_close  : 196.6043701171875\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_day_high   : 198.56590270996094\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_day_low    : 194.614990234375\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_day_open   : 196.99160766601562\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_week_high  : 198.56590270996094\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_week_low   : 194.614990234375\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     r2              : 0.9516183137893677\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     rmse            : 0.21295852628196713\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss [10] : (0.0478838917244766, 0.05583053156164766)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_loss [10]   : (0.03916197391509107, 0.04103451780974865)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : child_GOOG\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size  : 32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     context_len : 60\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs      : 10\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hidden_size : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     input_size  : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr          : 0.0003\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_layers  : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     parent_dir  : outputs/parent\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pred_len    : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     start       : 2000-01-01\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     ticker      : GOOG\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     asset                        : 3 (1.14 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     images                       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model-element                : 2 (1.28 MB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOG → MSE: 0.04535, RMSE: 0.21296, R²: 0.95162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for metadata to finish uploading (timeout is 3600 seconds)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 2 metrics, params and output messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GOOG model trained and saved to: outputs/GOOG\n",
      "✓ Predictions saved to: outputs/GOOG/GOOG_child_forecast.json\n",
      "✓ Metrics saved to: outputs/GOOG/GOOG_metrics.json\n",
      "\n",
      "3. Generating fresh predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GOOG predictions for 2025-08-26:\n",
      "  Next-day open: $196.99\n",
      "  Next-day high: $198.57\n",
      "  Next-day low: $194.61\n",
      "  Next-day close: $196.60\n",
      "  Next-week high: $198.57\n",
      "  Next-week low: $194.61\n",
      "\n",
      "==================================================\n",
      "Pipeline completed! Check 'outputs/' directory for:\n",
      "- Model checkpoints (model.pt files)\n",
      "- Scalers (scaler.pkl files)\n",
      "- Prediction JSONs (*_forecast.json)\n",
      "- Performance metrics (*_metrics.json)\n",
      "- Forecast plots (*_history_forecast.png)\n",
      "\n",
      "File structure:\n",
      "outputs/\n",
      "├── parent/\n",
      "│   ├── model.pt\n",
      "│   ├── scaler.pkl\n",
      "│   └── GSPC_metrics.json\n",
      "├── GOOG/\n",
      "│   ├── model.pt\n",
      "│   ├── scaler.pkl\n",
      "│   ├── GOOG_forecast.json\n",
      "│   ├── GOOG_metrics.json\n",
      "│   └── GOOG_history_forecast.png\n",
      "\n",
      "2. Training child model for AMZN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/karan-shingde/child-model/a1dd6fc8eaaf426d9c130e6a750b7f1f\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/Users/karan/Documents/machine-learning/everything-mlops/mlops-from-scrstch' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 0.12021\n",
      "Epoch 1/10 - Val Loss: 0.11875\n",
      "Epoch 2/10 - Train Loss: 0.11583\n",
      "Epoch 2/10 - Val Loss: 0.11769\n",
      "Epoch 3/10 - Train Loss: 0.11627\n",
      "Epoch 3/10 - Val Loss: 0.11755\n",
      "Epoch 4/10 - Train Loss: 0.11606\n",
      "Epoch 4/10 - Val Loss: 0.11738\n",
      "Epoch 5/10 - Train Loss: 0.11589\n",
      "Epoch 5/10 - Val Loss: 0.11687\n",
      "Epoch 6/10 - Train Loss: 0.11571\n",
      "Epoch 6/10 - Val Loss: 0.11690\n",
      "Epoch 7/10 - Train Loss: 0.11541\n",
      "Epoch 7/10 - Val Loss: 0.11670\n",
      "Epoch 8/10 - Train Loss: 0.11557\n",
      "Epoch 8/10 - Val Loss: 0.11682\n",
      "Epoch 9/10 - Train Loss: 0.11546\n",
      "Epoch 9/10 - Val Loss: 0.11691\n",
      "Epoch 10/10 - Train Loss: 0.11499\n",
      "Epoch 10/10 - Val Loss: 0.11702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : child_AMZN\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/karan-shingde/child-model/a1dd6fc8eaaf426d9c130e6a750b7f1f\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mse             : 0.1143484115600586\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_day_close  : 221.8444061279297\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_day_high   : 224.2028045654297\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_day_low    : 219.7496337890625\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_day_open   : 221.9628143310547\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_week_high  : 224.2028045654297\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_week_low   : 219.7496337890625\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     r2              : 0.883383572101593\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     rmse            : 0.33815441969617754\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss [10] : (0.11498763447161764, 0.12021069902693853)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_loss [10]   : (0.11669872892089188, 0.11875454299151897)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : child_AMZN\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size  : 32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     context_len : 60\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs      : 10\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hidden_size : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     input_size  : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr          : 0.0003\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_layers  : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     parent_dir  : outputs/parent\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pred_len    : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     start       : 2000-01-01\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     ticker      : AMZN\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     asset                        : 3 (1.14 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     images                       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model-element                : 2 (1.28 MB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMZN → MSE: 0.11435, RMSE: 0.33815, R²: 0.88338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ AMZN model trained and saved to: outputs/AMZN\n",
      "✓ Predictions saved to: outputs/AMZN/AMZN_child_forecast.json\n",
      "✓ Metrics saved to: outputs/AMZN/AMZN_metrics.json\n",
      "\n",
      "3. Generating fresh predictions...\n",
      "✓ GOOG predictions for 2025-08-26:\n",
      "  Next-day open: $196.99\n",
      "  Next-day high: $198.57\n",
      "  Next-day low: $194.61\n",
      "  Next-day close: $196.60\n",
      "  Next-week high: $198.57\n",
      "  Next-week low: $194.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ AMZN predictions for 2025-08-26:\n",
      "  Next-day open: $221.96\n",
      "  Next-day high: $224.20\n",
      "  Next-day low: $219.75\n",
      "  Next-day close: $221.84\n",
      "  Next-week high: $224.20\n",
      "  Next-week low: $219.75\n",
      "\n",
      "==================================================\n",
      "Pipeline completed! Check 'outputs/' directory for:\n",
      "- Model checkpoints (model.pt files)\n",
      "- Scalers (scaler.pkl files)\n",
      "- Prediction JSONs (*_forecast.json)\n",
      "- Performance metrics (*_metrics.json)\n",
      "- Forecast plots (*_history_forecast.png)\n",
      "\n",
      "File structure:\n",
      "outputs/\n",
      "├── parent/\n",
      "│   ├── model.pt\n",
      "│   ├── scaler.pkl\n",
      "│   └── GSPC_metrics.json\n",
      "├── GOOG/\n",
      "│   ├── model.pt\n",
      "│   ├── scaler.pkl\n",
      "│   ├── GOOG_forecast.json\n",
      "│   ├── GOOG_metrics.json\n",
      "│   └── GOOG_history_forecast.png\n",
      "├── AMZN/\n",
      "│   ├── model.pt\n",
      "│   ├── scaler.pkl\n",
      "│   ├── AMZN_forecast.json\n",
      "│   ├── AMZN_metrics.json\n",
      "│   └── AMZN_history_forecast.png\n",
      "\n",
      "2. Training child model for META...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/karan-shingde/child-model/612b5c5ab3dd4e53b9168eb5b249fa4c\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/Users/karan/Documents/machine-learning/everything-mlops/mlops-from-scrstch' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 0.09138\n",
      "Epoch 1/10 - Val Loss: 0.06091\n",
      "Epoch 2/10 - Train Loss: 0.08499\n",
      "Epoch 2/10 - Val Loss: 0.05920\n",
      "Epoch 3/10 - Train Loss: 0.08453\n",
      "Epoch 3/10 - Val Loss: 0.05897\n",
      "Epoch 4/10 - Train Loss: 0.08430\n",
      "Epoch 4/10 - Val Loss: 0.05833\n",
      "Epoch 5/10 - Train Loss: 0.08346\n",
      "Epoch 5/10 - Val Loss: 0.05792\n",
      "Epoch 6/10 - Train Loss: 0.08385\n",
      "Epoch 6/10 - Val Loss: 0.05817\n",
      "Epoch 7/10 - Train Loss: 0.08362\n",
      "Epoch 7/10 - Val Loss: 0.05739\n",
      "Epoch 8/10 - Train Loss: 0.08375\n",
      "Epoch 8/10 - Val Loss: 0.05731\n",
      "Epoch 9/10 - Train Loss: 0.08239\n",
      "Epoch 9/10 - Val Loss: 0.05727\n",
      "Epoch 10/10 - Train Loss: 0.08270\n",
      "Epoch 10/10 - Val Loss: 0.05733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : child_META\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/karan-shingde/child-model/612b5c5ab3dd4e53b9168eb5b249fa4c\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mse             : 0.07710161805152893\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_day_close  : 726.6943359375\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_day_high   : 736.3756713867188\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_day_low    : 717.3019409179688\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_day_open   : 728.9359741210938\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_week_high  : 736.3756713867188\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_week_low   : 717.3019409179688\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     r2              : 0.9097703099250793\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     rmse            : 0.2776717811581309\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss [10] : (0.08238721512848647, 0.09137678455288817)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_loss [10]   : (0.05727031507662365, 0.06090597764012359)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : child_META\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size  : 32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     context_len : 60\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs      : 10\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hidden_size : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     input_size  : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr          : 0.0003\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_layers  : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     parent_dir  : outputs/parent\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pred_len    : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     start       : 2000-01-01\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     ticker      : META\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     asset                        : 3 (1.14 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     images                       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model-element                : 2 (1.28 MB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "META → MSE: 0.07710, RMSE: 0.27767, R²: 0.90977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for assets to finish uploading (timeout is 10800 seconds)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m All assets have been sent, waiting for delivery confirmation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ META model trained and saved to: outputs/META\n",
      "✓ Predictions saved to: outputs/META/META_child_forecast.json\n",
      "✓ Metrics saved to: outputs/META/META_metrics.json\n",
      "\n",
      "3. Generating fresh predictions...\n",
      "✓ GOOG predictions for 2025-08-26:\n",
      "  Next-day open: $196.99\n",
      "  Next-day high: $198.57\n",
      "  Next-day low: $194.61\n",
      "  Next-day close: $196.60\n",
      "  Next-week high: $198.57\n",
      "  Next-week low: $194.61\n",
      "✓ AMZN predictions for 2025-08-26:\n",
      "  Next-day open: $221.96\n",
      "  Next-day high: $224.20\n",
      "  Next-day low: $219.75\n",
      "  Next-day close: $221.84\n",
      "  Next-week high: $224.20\n",
      "  Next-week low: $219.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ META predictions for 2025-08-26:\n",
      "  Next-day open: $728.94\n",
      "  Next-day high: $736.38\n",
      "  Next-day low: $717.30\n",
      "  Next-day close: $726.69\n",
      "  Next-week high: $736.38\n",
      "  Next-week low: $717.30\n",
      "\n",
      "==================================================\n",
      "Pipeline completed! Check 'outputs/' directory for:\n",
      "- Model checkpoints (model.pt files)\n",
      "- Scalers (scaler.pkl files)\n",
      "- Prediction JSONs (*_forecast.json)\n",
      "- Performance metrics (*_metrics.json)\n",
      "- Forecast plots (*_history_forecast.png)\n",
      "\n",
      "File structure:\n",
      "outputs/\n",
      "├── parent/\n",
      "│   ├── model.pt\n",
      "│   ├── scaler.pkl\n",
      "│   └── GSPC_metrics.json\n",
      "├── GOOG/\n",
      "│   ├── model.pt\n",
      "│   ├── scaler.pkl\n",
      "│   ├── GOOG_forecast.json\n",
      "│   ├── GOOG_metrics.json\n",
      "│   └── GOOG_history_forecast.png\n",
      "├── AMZN/\n",
      "│   ├── model.pt\n",
      "│   ├── scaler.pkl\n",
      "│   ├── AMZN_forecast.json\n",
      "│   ├── AMZN_metrics.json\n",
      "│   └── AMZN_history_forecast.png\n",
      "├── META/\n",
      "│   ├── model.pt\n",
      "│   ├── scaler.pkl\n",
      "│   ├── META_forecast.json\n",
      "│   ├── META_metrics.json\n",
      "│   └── META_history_forecast.png\n",
      "\n",
      "2. Training child model for AXP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/karan-shingde/child-model/92e512fe92de4fed81044bcdf96d7949\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/Users/karan/Documents/machine-learning/everything-mlops/mlops-from-scrstch' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 0.07360\n",
      "Epoch 1/10 - Val Loss: 0.06863\n",
      "Epoch 2/10 - Train Loss: 0.06723\n",
      "Epoch 2/10 - Val Loss: 0.06711\n",
      "Epoch 3/10 - Train Loss: 0.06662\n",
      "Epoch 3/10 - Val Loss: 0.06632\n",
      "Epoch 4/10 - Train Loss: 0.06605\n",
      "Epoch 4/10 - Val Loss: 0.06602\n",
      "Epoch 5/10 - Train Loss: 0.06579\n",
      "Epoch 5/10 - Val Loss: 0.06594\n",
      "Epoch 6/10 - Train Loss: 0.06591\n",
      "Epoch 6/10 - Val Loss: 0.06575\n",
      "Epoch 7/10 - Train Loss: 0.06518\n",
      "Epoch 7/10 - Val Loss: 0.06558\n",
      "Epoch 8/10 - Train Loss: 0.06542\n",
      "Epoch 8/10 - Val Loss: 0.06549\n",
      "Epoch 9/10 - Train Loss: 0.06558\n",
      "Epoch 9/10 - Val Loss: 0.06536\n",
      "Epoch 10/10 - Train Loss: 0.06548\n",
      "Epoch 10/10 - Val Loss: 0.06548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : child_AXP\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/karan-shingde/child-model/92e512fe92de4fed81044bcdf96d7949\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mse             : 0.06392543017864227\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_day_close  : 300.9759826660156\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_day_high   : 303.3474426269531\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_day_low    : 296.7518615722656\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_day_open   : 300.0487060546875\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_week_high  : 303.3474426269531\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     next_week_low   : 296.7518615722656\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     r2              : 0.9361895322799683\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     rmse            : 0.25283478830778466\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss [10] : (0.06518128196476028, 0.073601295001572)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_loss [10]   : (0.06536028180271387, 0.06863432731479406)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : child_AXP\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size  : 32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     context_len : 60\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs      : 10\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hidden_size : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     input_size  : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr          : 0.0003\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_layers  : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     parent_dir  : outputs/parent\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pred_len    : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     start       : 2000-01-01\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     ticker      : AXP\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     asset                        : 3 (1.14 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     images                       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model-element                : 2 (1.28 MB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AXP → MSE: 0.06393, RMSE: 0.25283, R²: 0.93619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 2 metrics, params and output messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ AXP model trained and saved to: outputs/AXP\n",
      "✓ Predictions saved to: outputs/AXP/AXP_child_forecast.json\n",
      "✓ Metrics saved to: outputs/AXP/AXP_metrics.json\n",
      "\n",
      "3. Generating fresh predictions...\n",
      "✓ GOOG predictions for 2025-08-26:\n",
      "  Next-day open: $196.99\n",
      "  Next-day high: $198.57\n",
      "  Next-day low: $194.61\n",
      "  Next-day close: $196.60\n",
      "  Next-week high: $198.57\n",
      "  Next-week low: $194.61\n",
      "✓ AMZN predictions for 2025-08-26:\n",
      "  Next-day open: $221.96\n",
      "  Next-day high: $224.20\n",
      "  Next-day low: $219.75\n",
      "  Next-day close: $221.84\n",
      "  Next-week high: $224.20\n",
      "  Next-week low: $219.75\n",
      "✓ META predictions for 2025-08-26:\n",
      "  Next-day open: $728.94\n",
      "  Next-day high: $736.38\n",
      "  Next-day low: $717.30\n",
      "  Next-day close: $726.69\n",
      "  Next-week high: $736.38\n",
      "  Next-week low: $717.30\n",
      "✓ AXP predictions for 2025-08-26:\n",
      "  Next-day open: $300.05\n",
      "  Next-day high: $303.35\n",
      "  Next-day low: $296.75\n",
      "  Next-day close: $300.98\n",
      "  Next-week high: $303.35\n",
      "  Next-week low: $296.75\n",
      "\n",
      "==================================================\n",
      "Pipeline completed! Check 'outputs/' directory for:\n",
      "- Model checkpoints (model.pt files)\n",
      "- Scalers (scaler.pkl files)\n",
      "- Prediction JSONs (*_forecast.json)\n",
      "- Performance metrics (*_metrics.json)\n",
      "- Forecast plots (*_history_forecast.png)\n",
      "\n",
      "File structure:\n",
      "outputs/\n",
      "├── parent/\n",
      "│   ├── model.pt\n",
      "│   ├── scaler.pkl\n",
      "│   └── GSPC_metrics.json\n",
      "├── GOOG/\n",
      "│   ├── model.pt\n",
      "│   ├── scaler.pkl\n",
      "│   ├── GOOG_forecast.json\n",
      "│   ├── GOOG_metrics.json\n",
      "│   └── GOOG_history_forecast.png\n",
      "├── AMZN/\n",
      "│   ├── model.pt\n",
      "│   ├── scaler.pkl\n",
      "│   ├── AMZN_forecast.json\n",
      "│   ├── AMZN_metrics.json\n",
      "│   └── AMZN_history_forecast.png\n",
      "├── META/\n",
      "│   ├── model.pt\n",
      "│   ├── scaler.pkl\n",
      "│   ├── META_forecast.json\n",
      "│   ├── META_metrics.json\n",
      "│   └── META_history_forecast.png\n",
      "├── AXP/\n",
      "│   ├── model.pt\n",
      "│   ├── scaler.pkl\n",
      "│   ├── AXP_forecast.json\n",
      "│   ├── AXP_metrics.json\n",
      "│   └── AXP_history_forecast.png\n"
     ]
    }
   ],
   "source": [
    " # Step 2: Train child models with transfer learning\n",
    "results = {}\n",
    "for ticker in CHILD_TICKERS:\n",
    "    print(f\"\\n2. Training child model for {ticker}...\")\n",
    "    try:\n",
    "        summary = train_child(\n",
    "            child_ticker=ticker,\n",
    "            start=START_DATE,\n",
    "            epochs=CHILD_EPOCHS,\n",
    "            parent_dir=parent_dir,\n",
    "            workdir=\"outputs\"\n",
    "        )\n",
    "        results[ticker] = summary\n",
    "        print(f\"✓ {ticker} model trained and saved to: {summary['checkpoint']}\")\n",
    "        print(f\"✓ Predictions saved to: {summary['json']}\")\n",
    "        print(f\"✓ Metrics saved to: {summary['checkpoint']}/{ticker}_metrics.json\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error training {ticker}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # # # Step 3: Generate fresh predictions\n",
    "    print(f\"\\n3. Generating fresh predictions...\")\n",
    "    for ticker in CHILD_TICKERS:\n",
    "        if ticker in results:\n",
    "            try:\n",
    "                preds = predict_child(ticker, parent_dir=parent_dir, workdir=\"outputs\")\n",
    "                print(f\"✓ {ticker} predictions for {preds['next_business_day']}:\")\n",
    "                print(f\"  Next-day open: ${preds['predictions']['next_day_open']:.2f}\")\n",
    "                print(f\"  Next-day high: ${preds['predictions']['next_day_high']:.2f}\")\n",
    "                print(f\"  Next-day low: ${preds['predictions']['next_day_low']:.2f}\")\n",
    "                print(f\"  Next-day close: ${preds['predictions']['next_day_close']:.2f}\")\n",
    "                print(f\"  Next-week high: ${preds['predictions']['next_week_high']:.2f}\")\n",
    "                print(f\"  Next-week low: ${preds['predictions']['next_week_low']:.2f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error predicting {ticker}: {e}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(\"Pipeline completed! Check 'outputs/' directory for:\")\n",
    "    print(\"- Model checkpoints (model.pt files)\")\n",
    "    print(\"- Scalers (scaler.pkl files)\")  \n",
    "    print(\"- Prediction JSONs (*_forecast.json)\")\n",
    "    print(\"- Performance metrics (*_metrics.json)\")\n",
    "    print(\"- Forecast plots (*_history_forecast.png)\")\n",
    "    print(\"\\nFile structure:\")\n",
    "    print(\"outputs/\")\n",
    "    print(\"├── parent/\")\n",
    "    print(f\"│   ├── model.pt\")\n",
    "    print(f\"│   ├── scaler.pkl\")\n",
    "    print(f\"│   └── {PARENT_TICKER.replace('^', '')}_metrics.json\")\n",
    "    for ticker in CHILD_TICKERS:\n",
    "        if ticker in results:\n",
    "            print(f\"├── {ticker}/\")\n",
    "            print(f\"│   ├── model.pt\")\n",
    "            print(f\"│   ├── scaler.pkl\") \n",
    "            print(f\"│   ├── {ticker}_forecast.json\")\n",
    "            print(f\"│   ├── {ticker}_metrics.json\")\n",
    "            print(f\"│   └── {ticker}_history_forecast.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d75a48b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Experiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 99\u001b[39m\n\u001b[32m     93\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_model\u001b[39m(model: nn.Module, loader: DataLoader, val_loader: DataLoader, epochs=\u001b[32m8\u001b[39m, lr=\u001b[32m1e-3\u001b[39m, experiment: Experiment = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    100\u001b[39m     model.to(DEVICE)\n\u001b[32m    101\u001b[39m     opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=\u001b[32m1e-4\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'Experiment' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76602706",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional, got ndarray of shape (1419, 1) instead",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = perform_eda(PARENT_TICKER, start=\u001b[33m\"\u001b[39m\u001b[33m2020-01-01\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mperform_eda\u001b[39m\u001b[34m(ticker, start, out_dir, model_type, experiment)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Add technical indicators\u001b[39;00m\n\u001b[32m     19\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mSMA_20\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mClose\u001b[39m\u001b[33m'\u001b[39m].rolling(window=\u001b[32m20\u001b[39m).mean()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mRSI_14\u001b[39m\u001b[33m'\u001b[39m] = ta.momentum.RSIIndicator(df[\u001b[33m'\u001b[39m\u001b[33mClose\u001b[39m\u001b[33m'\u001b[39m], window=\u001b[32m14\u001b[39m).rsi()\n\u001b[32m     21\u001b[39m bb = ta.volatility.BollingerBands(df[\u001b[33m'\u001b[39m\u001b[33mClose\u001b[39m\u001b[33m'\u001b[39m], window=\u001b[32m20\u001b[39m, window_dev=\u001b[32m2\u001b[39m)\n\u001b[32m     22\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mBB_High\u001b[39m\u001b[33m'\u001b[39m] = bb.bollinger_hband()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/venv-v2/lib/python3.12/site-packages/ta/momentum.py:34\u001b[39m, in \u001b[36mRSIIndicator.__init__\u001b[39m\u001b[34m(self, close, window, fillna)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mself\u001b[39m._window = window\n\u001b[32m     33\u001b[39m \u001b[38;5;28mself\u001b[39m._fillna = fillna\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28mself\u001b[39m._run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/venv-v2/lib/python3.12/site-packages/ta/momentum.py:48\u001b[39m, in \u001b[36mRSIIndicator._run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     44\u001b[39m emadn = down_direction.ewm(\n\u001b[32m     45\u001b[39m     alpha=\u001b[32m1\u001b[39m / \u001b[38;5;28mself\u001b[39m._window, min_periods=min_periods, adjust=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     46\u001b[39m ).mean()\n\u001b[32m     47\u001b[39m relative_strength = emaup / emadn\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28mself\u001b[39m._rsi = pd.Series(\n\u001b[32m     49\u001b[39m     np.where(emadn == \u001b[32m0\u001b[39m, \u001b[32m100\u001b[39m, \u001b[32m100\u001b[39m - (\u001b[32m100\u001b[39m / (\u001b[32m1\u001b[39m + relative_strength))),\n\u001b[32m     50\u001b[39m     index=\u001b[38;5;28mself\u001b[39m._close.index,\n\u001b[32m     51\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/venv-v2/lib/python3.12/site-packages/pandas/core/series.py:584\u001b[39m, in \u001b[36mSeries.__init__\u001b[39m\u001b[34m(self, data, index, dtype, name, copy, fastpath)\u001b[39m\n\u001b[32m    582\u001b[39m         data = data.copy()\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     data = sanitize_array(data, index, dtype, copy)\n\u001b[32m    586\u001b[39m     manager = _get_option(\u001b[33m\"\u001b[39m\u001b[33mmode.data_manager\u001b[39m\u001b[33m\"\u001b[39m, silent=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    587\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33mblock\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/venv-v2/lib/python3.12/site-packages/pandas/core/construction.py:656\u001b[39m, in \u001b[36msanitize_array\u001b[39m\u001b[34m(data, index, dtype, copy, allow_2d)\u001b[39m\n\u001b[32m    653\u001b[39m             subarr = cast(np.ndarray, subarr)\n\u001b[32m    654\u001b[39m             subarr = maybe_infer_to_datetimelike(subarr)\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m subarr = _sanitize_ndim(subarr, data, dtype, index, allow_2d=allow_2d)\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(subarr, np.ndarray):\n\u001b[32m    659\u001b[39m     \u001b[38;5;66;03m# at this point we should have dtype be None or subarr.dtype == dtype\u001b[39;00m\n\u001b[32m    660\u001b[39m     dtype = cast(np.dtype, dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/venv-v2/lib/python3.12/site-packages/pandas/core/construction.py:715\u001b[39m, in \u001b[36m_sanitize_ndim\u001b[39m\u001b[34m(result, data, dtype, index, allow_2d)\u001b[39m\n\u001b[32m    713\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m allow_2d:\n\u001b[32m    714\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m--> \u001b[39m\u001b[32m715\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    716\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData must be 1-dimensional, got ndarray of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m instead\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    717\u001b[39m     )\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[32m    719\u001b[39m     \u001b[38;5;66;03m# i.e. NumpyEADtype(\"O\")\u001b[39;00m\n\u001b[32m    721\u001b[39m     result = com.asarray_tuplesafe(data, dtype=np.dtype(\u001b[33m\"\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mValueError\u001b[39m: Data must be 1-dimensional, got ndarray of shape (1419, 1) instead"
     ]
    }
   ],
   "source": [
    "result = perform_eda(PARENT_TICKER, start=\"2020-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c89316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
